{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adf3ccf2-2eb6-4864-9c21-ab4df69c1c61",
   "metadata": {},
   "source": [
    "# [Langchain Callbacks](https://python.langchain.com/docs/modules/callbacks/)\n",
    "Le **callback** di Langchain ci permettono di ottenere lo *streaming* del processo di esecuzione di Open AI.<br/>\n",
    "Lo stdout handler di default che introduciamo oggi semplicemente \"logga\" tutti gli eventi sullo standard output. Questo standard handler è implementato nelle librerie di Langchain sotto *conda_env/Lib/site-packages/langchain/callbacks/stdout.py*.<br/>\n",
    "\n",
    "## [Callbacks documentation](https://python.langchain.com/docs/modules/callbacks/)\n",
    "\n",
    "**Callbacks**:<br>\n",
    "LangChain provides a callbacks system that allows you to hook into the various stages of your LLM application. This is useful for logging, monitoring, streaming, and other tasks. You can subscribe to these events by using the callbacks argument available throughout the API. \n",
    "\n",
    "**Callback handlers**:<br>\n",
    "CallbackHandlers are objects that implement the **CallbackHandler** interface, which has a method for each event that can be subscribed to. The CallbackManager will call the appropriate method on each handler when the event is triggered.<br/>\n",
    "Nel nostro esempio abbiamo la possibilità di utilizzare l'interfaccia di CallbackHandler implementata nella classe `StdOutCallbackHandler` di  `common/callbacks.py`, però per capire ancora meglio implementeremo in una cella questa specifica interfaccia. Tale classe deriva da `langchain.callbacks.base`.\n",
    "\n",
    "---------------\n",
    "We will incorporate a handler for the callbacks, enabling us to observe the response as it streams and to gain insights into the Agent's reasoning process. This will prove incredibly valuable when we aim to stream the bot's responses to users and keep them informed about the ongoing process as they await the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2604f8-acc4-4db2-ab86-196a550753a3",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22959573-ca80-40df-a1d1-04e9d968e90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "# load AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_API_KEY, OPENAI_API_VERSION and AZURE_OPENAI_API_TYPE\n",
    "# plus COMPLETION4_DEPLOYMENT, to be assigned to the MODEL string\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "MODEL = os.environ[\"GPT4-0613-8k\"] \n",
    "\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3643bc7-333e-4572-b550-3b801dc6b623",
   "metadata": {},
   "source": [
    "## Let's start with Langchain HelloWorld with a normal LLM\n",
    "### No handlers here for the moment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b634818-e9fa-4500-b870-a0f7330bd5fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer the following question: \"What is CLP?\". Give your response in Italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is CLP?',\n",
       " 'language': 'Italian',\n",
       " 'text': \"CLP è l'acronimo di Chilean Peso, la valuta del Cile.\"}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Now we create a simple prompt template\n",
    "QUESTION = \"What is CLP?\" # What are the approaches to Task Decomposition?\n",
    "language = \"Italian\" # German\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"question\", \"language\"],\n",
    "    template='Answer the following question: \"{question}\". Give your response in {language}',\n",
    ")\n",
    "\n",
    "# Let's print the question using Langchain prompt\n",
    "print(prompt.format(question=QUESTION, language=language))\n",
    "\n",
    "# And finally we create our first generic chain\n",
    "chain_chat1 = LLMChain(llm=llm, prompt=prompt, verbose=False) # verbose=False is the default\n",
    "chain_chat1({\"question\": QUESTION, \"language\": language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9174375e-224b-4d2c-86c4-232de17bd519",
   "metadata": {},
   "source": [
    "### Now we ask the same question, using the default standard handler (StdOutCallbackHandler)\n",
    "**Please note**: rather than `callbacks=[standard_handler]` we could write `verbose=True`. So in this case `verbose=False` is ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28bdbf38-6e48-4d56-a0f2-c3ee80053c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mAnswer the following question: \"What is CLP?\". Give your response in Italian\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What is CLP?',\n",
       " 'language': 'Italian',\n",
       " 'text': \"CLP è l'acronimo di Chilean Peso, la valuta del Cile.\"}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "standard_handler = StdOutCallbackHandler()\n",
    "chain_chat2 = LLMChain(llm=llm, prompt=prompt, verbose=True)# False, callbacks=[standard_handler])\n",
    "chain_chat2({\"question\": QUESTION, \"language\": language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1392fd95-c50f-4555-80cc-214dfd1e5ab3",
   "metadata": {},
   "source": [
    "## CUSTOM Callback Handler\n",
    "I simply duplicate the `StdOutCallbackHandler` class of `conda_env/Lib/site-packages/langchain/callbacks/stdout.py` and customize `on_chain_start` to *speak* Italian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa41a548-f8da-4a89-8390-8f7a12aa4d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.schema import AgentAction, AgentFinish, LLMResult\n",
    "\n",
    "class StdOutCallbackHandler_custom(BaseCallbackHandler):\n",
    "    \"\"\"Callback handler for streaming in agents.\n",
    "    Only works with agents using LLMs that support streaming.\n",
    "    \"\"\"\n",
    "    \n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        \"\"\"Run on new LLM token. Only available when streaming is enabled.\"\"\"\n",
    "        message = f\"\\n\\n\\033[1m> Sto analizzando il nuovo token {token}...\\033[0m\" # <<<--- ITALIAN\n",
    "        print(message) # <<<<<<<< ITALIAN!\n",
    "        sys.stdout.write(token)\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    def on_llm_error(self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any) -> Any:\n",
    "        \"\"\"Run when LLM errors.\"\"\"\n",
    "        sys.stdout.write(f\"LLM Error: {error}\\n\")\n",
    "\n",
    "    def on_chain_start(self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any) -> Any:\n",
    "        \"\"\"Print out that we are entering a chain.\"\"\"\n",
    "        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n",
    "        message = f\"\\n\\n\\033[1m> Sto entrando nella catena {class_name}...\\033[0m\" # <<<--- ITALIAN\n",
    "        print(message) # <<<<<<<< ITALIAN!\n",
    "        sys.stdout.write(message.upper())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    def on_tool_start(self, serialized: Dict[str, Any], input_str: str, **kwargs: Any) -> Any:\n",
    "        sys.stdout.write(f\"Tool: {serialized['name']}\\n\")\n",
    "        class_name = serialized.get(\"name\", serialized.get(\"id\", [\"<unknown>\"])[-1])\n",
    "        message = f\"\\n\\n\\033[1m> Sto avviando il TOOL per {class_name}...\\033[0m\"\n",
    "        print(message) # <<<<<<<< ITALIAN!\n",
    "        sys.stdout.write(message.upper())\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "    def on_agent_action(self, action: AgentAction, **kwargs: Any) -> Any:\n",
    "        sys.stdout.write(f\"{action.log}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bed8a3-04cc-4a36-b00a-b5365c7b8ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "custom_handler = StdOutCallbackHandler_custom()\n",
    "custom_cb_manager = CallbackManager(handlers=[custom_handler])\n",
    "chain_chat3 = LLMChain(llm=llm, prompt=prompt, verbose=False, callback_manager=custom_cb_manager)\n",
    "chain_chat3({\"question\": QUESTION, \"language\": language})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc933d-1051-4952-a0aa-a84d7a4bbace",
   "metadata": {},
   "source": [
    "## Let's add [*streaming*](https://python.langchain.com/docs/modules/model_io/models/llms/streaming_llm) now!\n",
    "- Upon receiving the client’s request, the server starts sending the data back in chunks or as a continuous stream. It doesn't wait for the entire response to be generated before sending it.\n",
    "- As the server sends the data in chunks, the client can start receiving and processing the received data immediately. This enables real-time updates and allows the client to display or act upon the received information as it arrives.\n",
    "- The connection remains open: Unlike traditional HTTP requests where the connection is closed after the server sends the response, in HTTP streaming, the connection remains open. This allows the server to continue sending data to the client over the same connection.\n",
    "- Continuous data delivery: The server keeps sending data to the client as it becomes available or as per a defined interval. This enables a continuous flow of data from the server to the client, similar to a stream of water flowing steadily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ae9a5-cd43-44f5-a412-c0dfc88c437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.callbacks.manager import CallbackManager\n",
    "\n",
    "llm_streaming = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=1000, streaming=True)\n",
    "\n",
    "chain_chat4 = LLMChain(llm=llm_streaming, prompt=prompt, verbose=True, callback_manager=custom_cb_manager)\n",
    "\n",
    "chain_chat4({\"question\": QUESTION, \"language\": language})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5078ee8-065c-446e-9461-a86dc4c26f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = chain_chat4({\"question\": \"Write me a song about sparkling water.\", \"language\": language})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83e926-3525-4b8b-9d3e-734f3012e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in resp:\n",
    "    key=chunk\n",
    "    value=resp[key]\n",
    "    print(f\"*** {key}: ***\\n{value}\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azuremlsdkv2mm",
   "language": "python",
   "name": "azuremlsdkv2mm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
