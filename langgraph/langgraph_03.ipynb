{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0302cd-4476-496f-8717-b1919edff95f",
   "metadata": {},
   "source": [
    "# Load Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96bd0fe9-61f0-40b4-92aa-3c9445787d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = os.environ[\"AZURE_OPENAI_ENDPOINT_SCUS\"]\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"]  = os.environ[\"AZURE_OPENAI_API_KEY_SCUS\"]\n",
    "os.environ[\"OPENAI_API_VERSION\"]    = os.environ[\"AZURE_OPENAI_API_VERSION\"]\n",
    "os.environ[\"AZURE_OPENAI_API_TYPE\"] = os.environ[\"OPENAI_API_TYPE\"]\n",
    "\n",
    "MODEL = os.environ[\"GPT4-0125PREVIEW-128k\"]\n",
    "\n",
    "# https://smith.langchain.com/\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]  = \"false\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]     = \"langgraph_02 Custom Agents\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]     = os.environ[\"LANGCHAIN_API_KEY\"]\n",
    "\n",
    "question_basic                      = \"What's the date of Easter 2021?\"\n",
    "question_for_tools                  = \"My name is Mauro. What do I get, if I apply the magic tool to the number of characters of my name?\"\n",
    "question_follow_up                  = \"Is this result an even or odd number??\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d3f7f4-99b9-4265-9b15-c05488dc34cd",
   "metadata": {},
   "source": [
    "## Automate a [Tool Calling Agent](https://python.langchain.com/docs/modules/agents/agent_types/tool_calling/) creation\n",
    "Let's recall that this agent has the following three requirements:\n",
    "1) One or more functions, each one associated to a tool, all included in a `tools` list\n",
    "2) An LLM\n",
    "3) A ChatPromptTemplate that includes a `MessagesPlaceholder(\"agent_scratchpad\")`\n",
    "\n",
    "Now we create 1) and 2) which are common to every agent.<br/>\n",
    "For the implementation of 3), the only custom component is the system message so we create an helper function called `create_agent_executor()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e438a19-f656-4935-8c2c-a77df47f9aff",
   "metadata": {},
   "source": [
    "### Automate a Tool Calling Agent creation --> Create the tools this agent has access to\n",
    "- Create one or more functions\n",
    "- Decorate each function as a tool\n",
    "- Create a list containing all tools defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "253936b3-6764-41c4-a912-2741d43bbdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some tools, then add them to the \"tools\" list\n",
    "\n",
    "from langchain.tools import tool\n",
    "@tool(\"internet_search_tool\", return_direct=False) # this is the name tracked in LangSmith\n",
    "def internet_search_function(query: str) -> str:\n",
    "    \"\"\"Searches the internet using DuckDuckGo.\"\"\"\n",
    "    from duckduckgo_search import DDGS\n",
    "    with DDGS() as ddgs:\n",
    "        results = [r for r in ddgs.text(query, max_results=5)]\n",
    "        return results if results else \"No results found.\"\n",
    "\n",
    "@tool(\"process_content_tool\", return_direct=False)\n",
    "def process_content_function(url: str) -> str:\n",
    "    \"\"\"Processes content from a webpage.\"\"\"\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.get_text()\n",
    "\n",
    "tools = [internet_search_function, process_content_function]\n",
    "\n",
    "Q = \"who is the sister o Joe Biden?\"\n",
    "first_url = internet_search_function(Q)[0]['href']\n",
    "content = process_content_function(first_url)\n",
    "\n",
    "# print(content) # just for testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d83e95-eb21-4c98-805b-f7ff1b65f686",
   "metadata": {},
   "source": [
    "### Automate a Tool Calling Agent creation --> Create the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d152e040-59c4-41d2-b08e-e06c54febf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LLM object\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(deployment_name=MODEL, temperature=0, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9aeee5-b3e0-4510-ba3b-8f4b548e8991",
   "metadata": {},
   "source": [
    "### Create the \"Tool Calling Agent\" --> Create the ChatPromptTemplate object through an helper function\n",
    "**Important**: `Tool Calling Agent` requires an additional variable called `agent_scratchpad`. Intermediate agent actions and tool output messages will be passed in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8432f2b4-819a-42dc-9b3e-84273ba62d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for creating agents\n",
    "def create_agent_executor(llm: AzureChatOpenAI, tools: list, system_prompt: str):\n",
    "    from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "    from langchain.agents import AgentExecutor\n",
    "    from langchain.agents.openai_tools.base import create_openai_tools_agent\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ])\n",
    "    agent = create_openai_tools_agent(llm, tools, prompt)\n",
    "    agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "    return agent_executor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc277f-6bb1-4770-b398-a4402be0fd98",
   "metadata": {},
   "source": [
    "## Create the **Search Agent Executor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ab92c3a-c44b-4bad-ba7f-c68afac31298",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_agent_exec = create_agent_executor(\n",
    "    system_prompt=\"You are a web searcher. Search the internet for information.\",\n",
    "    llm=llm,\n",
    "    tools=tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acb8b67-ef33-42d4-a4a6-e95dc6b52694",
   "metadata": {},
   "source": [
    "## Create the **Insights Research Agent Executor**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4367243d-ffb1-4b99-8b57-bd32985f2453",
   "metadata": {},
   "outputs": [],
   "source": [
    "insights_research_agent_exec = create_agent_executor(\n",
    "    system_prompt = \"\"\"You are a Insight Researcher. Do step by step. \n",
    "        Based on the provided content first identify the list of topics,\n",
    "        then search internet for each topic one by one\n",
    "        and finally find insights for each topic one by one.\n",
    "        Include the insights and sources in the final response\"\"\",\n",
    "    llm = llm,\n",
    "    tools = tools\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80387b20-986d-49e8-8093-0b2b8dd7d8f9",
   "metadata": {},
   "source": [
    "# NODES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5907318a-a7e5-413b-853f-124e1fd3a8be",
   "metadata": {},
   "source": [
    "## Helper function to build a node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc60b95-8456-4a92-974f-b43061dc108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function facilitates creating nodes in the graph:\n",
    "# it takes care of invoking the agent executor and then converts its response to a human message. \n",
    "# That is how we will add it the global state of the graph\n",
    "\n",
    "def agent_node(state, agent_executor, node_name):\n",
    "    from langchain_core.messages import HumanMessage\n",
    "    result = agent_executor.invoke(state)\n",
    "    return {\"messages\": [HumanMessage(content=result[\"output\"], name=node_name)]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c370c5-e837-4358-bb21-66247af8a785",
   "metadata": {},
   "source": [
    "## Node creation function for the \"Web_Searcher\" agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36987881-dec9-4c89-a54b-19dac7065106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the search node create function, starting from the search agent executor\n",
    "\n",
    "import functools\n",
    "\n",
    "search_node_create_function = functools.partial(agent_node, agent_executor=search_agent_exec, name=\"Web_Searcher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709844d3-57f7-4ba0-92c5-3aca3160c5d9",
   "metadata": {},
   "source": [
    "## Node creation function for the \"Insights_Researcher\" agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d71bec2-b99a-4889-971d-7ad1e20f9aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the search node create function, starting from the search agent executor\n",
    "\n",
    "import functools\n",
    "\n",
    "researcher_node_create_function = functools.partial(agent_node, agent_executor=insights_research_agent_exec, name=\"Insights_Researcher\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5705d4-a4e3-4d9c-86b1-eb15a80d5b40",
   "metadata": {},
   "source": [
    "## Create the Supervisor Agent as a Custom Agent for Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0d1479-ef95-44c0-8f30-9743b225afe9",
   "metadata": {},
   "source": [
    "### Build Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40aef4b2-fdd6-497e-8c0f-5bf9230832a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "members: ['Web_Searcher', 'Insights_Researcher'],\n",
      "options: ['FINISH', 'Web_Searcher', 'Insights_Researcher']\n"
     ]
    }
   ],
   "source": [
    "# given the two nodes...\n",
    "members = [\"Web_Searcher\", \"Insights_Researcher\"]\n",
    "\n",
    "# ...the potential actions identified by the supervisor include \"FINISH\"\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "print(f\"members: {members},\\noptions: {options}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39a2634f-5838-401b-a4e6-44bf8d6166c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'route',\n",
       "  'description': 'Select the next role.',\n",
       "  'parameters': {'title': 'routeSchema',\n",
       "   'type': 'object',\n",
       "   'properties': {'next': {'title': 'Next',\n",
       "     'anyOf': [{'enum': ['FINISH', 'Web_Searcher', 'Insights_Researcher']}]}},\n",
       "   'required': ['next']}}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using openai function calling can make output parsing easier for us\n",
    "\n",
    "function_def = {\n",
    "    \"name\": \"route\",\n",
    "    \"description\": \"Select the next role.\",\n",
    "    \"parameters\": {\n",
    "        \"title\": \"routeSchema\",\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"next\": {\n",
    "                \"title\": \"Next\",\n",
    "                \"anyOf\": [\n",
    "                    {\"enum\": options},\n",
    "                ],\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"next\"],\n",
    "    },\n",
    "}\n",
    "\n",
    "tools = [function_def]\n",
    "\n",
    "tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3374d674-affb-421f-a005-ca9ab5fefdb4",
   "metadata": {},
   "source": [
    "### LLM \"bound\" to the tools above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "252e4a9f-78c0-4de3-b635-8dd5a2ef154c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fbe1e0e79d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fbe1e31b6d0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1000, azure_endpoint='https://mmopenaiscus.openai.azure.com/', deployment_name='gpt4-0125preview-128k', openai_api_version='2024-02-15-preview', openai_api_type='azure'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Web_Searcher', 'Insights_Researcher']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To pass in our tools to the agent, we just need to format them to the OpenAI tool format and pass them to our model. \n",
    "# By bind-ing the functions, we’re making sure that they’re passed in each time the model is invoked.\n",
    "\n",
    "# llm_bound_to_tools = llm.bind_tools(tools=tools, tool_choice=\"route\")\n",
    "llm_bound_to_tools = llm.bind_functions(functions=tools, function_call=\"route\")\n",
    "llm_bound_to_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242e0bee-b58e-4318-902c-73cd4fda0b86",
   "metadata": {},
   "source": [
    "### cpt_supervisor creation\n",
    "\n",
    "This ChatPromptTemplate called `cpt_supervisor` would actually take three input variables: `members`, `messages` and `options`.<br/>\n",
    "However, by using the `partial` function, we hard-code two variables (`members` and `options`), so that only `messages` is expected when invoking this template.<br/>\n",
    "The `partial()` method in this case is used to create a version of the template with some variables pre-filled, which reduces the number of variables that need to be provided when the template is used later.<br/>\n",
    "This effectively means that `members` and `options` do not need to be provided as input variables when using the second object, as they are already included as part of the template. More specifically, they are automatically passed with these values (both string types):\n",
    "- `members` --> \"'Web_Searcher', 'Insights_Researcher'\"\n",
    "- `options` --> \"'FINISH', 'Web_Searcher', 'Insights_Researcher'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "32dca884-b659-4e58-81bc-b9c2fc1d98be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Web_Searcher', 'Insights_Researcher']\", 'members': 'Web_Searcher, Insights_Researcher'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['members'], template='You are a supervisor tasked with managing a conversation between the following workers: {members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt_supervisor = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers: {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\")\n",
    "\n",
    "mp1 = MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "mp2 = (\n",
    "    \"system\",\n",
    "    \"Given the conversation above, who should act next?\"\n",
    "    \" Or should we FINISH? Select one of: {options}\")\n",
    "\n",
    "cpt_supervisor = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt_supervisor),\n",
    "        mp1,\n",
    "        mp2\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "cpt_supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d435fe5f-c429-4e18-9ba9-46070c21c7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a supervisor tasked with managing a conversation between the following workers: Web_Searcher, Insights_Researcher. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.'), HumanMessage(content='Hi!'), AIMessage(content='How can I assist you today?'), SystemMessage(content=\"Given the conversation above, who should act next? Or should we FINISH? Select one of: ['FINISH', 'Web_Searcher', 'Insights_Researcher']\")])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is just to demonstrate that cpt_supervisor may be called just passing the \"messages\" variable\n",
    "\n",
    "cpt_supervisor.invoke ({\"messages\": [(\"human\", \"Hi!\"), (\"ai\", \"How can I assist you today?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6136de1-3d5a-424e-bee5-3dcb8ba610f2",
   "metadata": {},
   "source": [
    "## Create the **supervisor** agent leveraging the objects collected above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1742c964-551c-40b7-971c-3246e9b74b23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['messages'], input_types={'messages': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, partial_variables={'options': \"['FINISH', 'Web_Searcher', 'Insights_Researcher']\", 'members': 'Web_Searcher, Insights_Researcher'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['members'], template='You are a supervisor tasked with managing a conversation between the following workers: {members}. Given the following user request, respond with the worker to act next. Each worker will perform a task and respond with their results and status. When finished, respond with FINISH.')), MessagesPlaceholder(variable_name='messages'), SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['options'], template='Given the conversation above, who should act next? Or should we FINISH? Select one of: {options}'))])\n",
       "| RunnableBinding(bound=AzureChatOpenAI(client=<openai.resources.chat.completions.Completions object at 0x7fbe1e0e79d0>, async_client=<openai.resources.chat.completions.AsyncCompletions object at 0x7fbe1e31b6d0>, temperature=0.0, openai_api_key=SecretStr('**********'), openai_proxy='', max_tokens=1000, azure_endpoint='https://mmopenaiscus.openai.azure.com/', deployment_name='gpt4-0125preview-128k', openai_api_version='2024-02-15-preview', openai_api_type='azure'), kwargs={'functions': [{'name': 'route', 'description': 'Select the next role.', 'parameters': {'title': 'routeSchema', 'type': 'object', 'properties': {'next': {'title': 'Next', 'anyOf': [{'enum': ['FINISH', 'Web_Searcher', 'Insights_Researcher']}]}}, 'required': ['next']}}], 'function_call': {'name': 'route'}})\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We're ready to put together the three elements needed to create the node as a pipeline:\n",
    "# - ChatPromptTemplate\n",
    "# - RunnableBinding\n",
    "# - OpenAIToolsAgentOutputParser\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "\n",
    "supervisor_agent = (\n",
    "    cpt_supervisor\n",
    "    | llm_bound_to_tools\n",
    "    | OpenAIToolsAgentOutputParser()   \n",
    ")\n",
    "\n",
    "supervisor_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe51f5f8-0132-406b-bf0e-8e386de090b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentFinish(return_values={'output': ''}, log='')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supervisor_agent.invoke ({\"messages\": [(\"human\", \"Hi!\"), (\"ai\", \"How can I assist you today?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ed518-17fb-4de1-8266-92cd3f147282",
   "metadata": {},
   "source": [
    "# GRAPH!\n",
    "\n",
    "Now that we have Tools, Agents and Nodes we are ready to build the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "38e94ef7-7cab-41f3-9fb9-73e27012c3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to define the \"AgentState\", which represents the conversation history\n",
    "from langchain_core.messages import BaseMessage\n",
    "from typing import Annotated, Any, Dict, List, Optional, Sequence, TypedDict\n",
    "import operator\n",
    "\n",
    "class AgentState(TypedDict):    \n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    next: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eedb14ef-b6fd-4a03-813f-cbb6bb49503b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, we create the graph using from langgraph.graph import StateGraph\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "workflow = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d14ec12-e915-479a-a88b-f13a6ce50741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Web_Searcher', 'Insights_Researcher', 'Supervisor']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To start building the workflow, we add the three nodes\n",
    "\n",
    "workflow.add_node(\"Web_Searcher\", search_node_create_function) # functools.partial\n",
    "workflow.add_node(\"Insights_Researcher\", researcher_node_create_function) # functools.partial\n",
    "workflow.add_node(\"Supervisor\", supervisor_agent) # langchain_core.runnables.base.RunnableSequence\n",
    "\n",
    "[n for n in workflow.nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7b4be7a-dc76-4556-a254-06db6080d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "connecting Web_Searcher node to the supervisor node...\n",
      "connecting Insights_Researcher node to the supervisor node...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Web_Searcher', 'Supervisor'), ('Insights_Researcher', 'Supervisor')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now we add the edges between the two node members and the supervisor\n",
    "# We want our workers to ALWAYS \"report back\" to the Supervisor node when done\n",
    "\n",
    "for member in members:\n",
    "    print(f\"connecting {member} node to the supervisor node...\")\n",
    "    workflow.add_edge(member, \"Supervisor\")\n",
    "    \n",
    "[e for e in workflow.edges]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1bb9028-0fc8-4b77-8c2b-8e0741b1132c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Web_Searcher': 'Web_Searcher',\n",
       " 'Insights_Researcher': 'Insights_Researcher',\n",
       " 'FINISH': '__end__'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The conditional map illustrates the match between each node and its direction\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "\n",
    "conditional_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2da94408-2b60-4205-9e1a-6c05cc6d3ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate the conditional map to the workflow\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"Supervisor\", \n",
    "    lambda x: x[\"next\"], \n",
    "    conditional_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "adf86080-968c-43ea-9691-43cdfffdc598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add entry point\n",
    "\n",
    "workflow.set_entry_point(\"Supervisor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8e293302-4bfd-4336-9e95-6db14163114a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CompiledStateGraph(nodes={'__start__': PregelNode(config={'tags': ['langsmith:hidden']}, channels=['__start__'], triggers=['__start__'], writers=[ChannelWrite<messages,next>(writes=[ChannelWriteEntry(channel='messages', value=_get_state_key(), skip_none=False), ChannelWriteEntry(channel='next', value=_get_state_key(), skip_none=False)]), ChannelWrite<start:Supervisor>(writes=[ChannelWriteEntry(channel='start:Supervisor', value='__start__', skip_none=False)])]), 'Web_Searcher': PregelNode(config={'tags': []}, channels={'messages': 'messages', 'next': 'next'}, triggers=['branch:Supervisor:condition:Web_Searcher'], mapper=functools.partial(<function _coerce_state at 0x7fbe175f9d00>, <class '__main__.AgentState'>), writers=[ChannelWrite<Web_Searcher,messages,next>(writes=[ChannelWriteEntry(channel='Web_Searcher', value=None, skip_none=False), ChannelWriteEntry(channel='messages', value=_get_state_key(), skip_none=False), ChannelWriteEntry(channel='next', value=_get_state_key(), skip_none=False)])]), 'Insights_Researcher': PregelNode(config={'tags': []}, channels={'messages': 'messages', 'next': 'next'}, triggers=['branch:Supervisor:condition:Insights_Researcher'], mapper=functools.partial(<function _coerce_state at 0x7fbe175f9d00>, <class '__main__.AgentState'>), writers=[ChannelWrite<Insights_Researcher,messages,next>(writes=[ChannelWriteEntry(channel='Insights_Researcher', value=None, skip_none=False), ChannelWriteEntry(channel='messages', value=_get_state_key(), skip_none=False), ChannelWriteEntry(channel='next', value=_get_state_key(), skip_none=False)])]), 'Supervisor': PregelNode(config={'tags': []}, channels={'messages': 'messages', 'next': 'next'}, triggers=['Web_Searcher', 'Insights_Researcher', 'start:Supervisor'], mapper=functools.partial(<function _coerce_state at 0x7fbe175f9d00>, <class '__main__.AgentState'>), writers=[ChannelWrite<Supervisor,messages,next>(writes=[ChannelWriteEntry(channel='Supervisor', value=None, skip_none=False), ChannelWriteEntry(channel='messages', value=_get_state_key(), skip_none=False), ChannelWriteEntry(channel='next', value=_get_state_key(), skip_none=False)]), _route(_is_channel_writer=True)])}, channels={'messages': <langgraph.channels.binop.BinaryOperatorAggregate object at 0x7fbe1751b7d0>, 'next': <langgraph.channels.last_value.LastValue object at 0x7fbe1754b4d0>, '__start__': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe17518d50>, 'Web_Searcher': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe175df910>, 'Insights_Researcher': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe17417390>, 'Supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe17417850>, 'start:Supervisor': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe17417c90>, 'branch:Supervisor:condition:Web_Searcher': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe1742c090>, 'branch:Supervisor:condition:Insights_Researcher': <langgraph.channels.ephemeral_value.EphemeralValue object at 0x7fbe1742c1d0>}, auto_validate=False, stream_mode='updates', output_channels=['messages', 'next'], stream_channels=['messages', 'next'], input_channels='__start__', graph=<langgraph.graph.state.StateGraph object at 0x7fbe1c114c90>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the graph\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6034e89-857a-42df-bb64-2a610a220e53",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidUpdateError",
     "evalue": "Expected dict, got return_values={'output': ''} log=''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidUpdateError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Run the graph\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HumanMessage\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[38;5;124;43mSearch for the latest AI technology trends in 2024,\u001b[39;49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;43m            summarize the content. After summarise pass it on to insight researcher\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;43m            to provide insights for each topic\u001b[39;49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__end__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:686\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, input_keys, interrupt_before_nodes, interrupt_after_nodes, debug)\u001b[0m\n\u001b[1;32m    679\u001b[0m done, inflight \u001b[38;5;241m=\u001b[39m concurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mwait(\n\u001b[1;32m    680\u001b[0m     futures,\n\u001b[1;32m    681\u001b[0m     return_when\u001b[38;5;241m=\u001b[39mconcurrent\u001b[38;5;241m.\u001b[39mfutures\u001b[38;5;241m.\u001b[39mFIRST_EXCEPTION,\n\u001b[1;32m    682\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[1;32m    683\u001b[0m )\n\u001b[1;32m    685\u001b[0m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[0;32m--> 686\u001b[0m \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minflight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;66;03m# combine pending writes from all tasks\u001b[39;00m\n\u001b[1;32m    689\u001b[0m pending_writes \u001b[38;5;241m=\u001b[39m deque[\u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]()\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1033\u001b[0m, in \u001b[0;36m_panic_or_proceed\u001b[0;34m(done, inflight, step)\u001b[0m\n\u001b[1;32m   1031\u001b[0m             inflight\u001b[38;5;241m.\u001b[39mpop()\u001b[38;5;241m.\u001b[39mcancel()\n\u001b[1;32m   1032\u001b[0m         \u001b[38;5;66;03m# raise the exception\u001b[39;00m\n\u001b[0;32m-> 1033\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[1;32m   1034\u001b[0m         \u001b[38;5;66;03m# TODO this is where retry of an entire step would happen\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inflight:\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;66;03m# if we got here means we timed out\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/concurrent/futures/thread.py:58\u001b[0m, in \u001b[0;36m_WorkItem.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfuture\u001b[38;5;241m.\u001b[39mset_exception(exc)\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:2499\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   2498\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps):\n\u001b[0;32m-> 2499\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2500\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2501\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# mark each step as a child run\u001b[39;49;00m\n\u001b[1;32m   2502\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2503\u001b[0m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseq:step:\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2504\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2505\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2506\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   2507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/utils.py:45\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace:\n\u001b[0;32m---> 45\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m         ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;28minput\u001b[39m, merge_configs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, config), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/base.py:1625\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, **kwargs)\u001b[0m\n\u001b[1;32m   1621\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1622\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(var_child_runnable_config\u001b[38;5;241m.\u001b[39mset, child_config)\n\u001b[1;32m   1623\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1624\u001b[0m         Output,\n\u001b[0;32m-> 1625\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1626\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1627\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1628\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1629\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1630\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1631\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1632\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1633\u001b[0m     )\n\u001b[1;32m   1634\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1635\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langchain_core/runnables/config.py:347\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    346\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 347\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/write.py:63\u001b[0m, in \u001b[0;36mChannelWrite._write\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mRunnable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchan\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrites\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     74\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m         write\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write, (_, _, skip_none) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites)\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_none \u001b[38;5;129;01mor\u001b[39;00m write[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     ]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(values))\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/pregel/write.py:66\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_write\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Any, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     64\u001b[0m         (\n\u001b[1;32m     65\u001b[0m             chan,\n\u001b[0;32m---> 66\u001b[0m             \u001b[43mr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(r, Runnable)\n\u001b[1;32m     68\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m r\n\u001b[1;32m     69\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28minput\u001b[39m,\n\u001b[1;32m     71\u001b[0m         )\n\u001b[1;32m     72\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m chan, r, _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites\n\u001b[1;32m     73\u001b[0m     ]\n\u001b[1;32m     74\u001b[0m     values \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     75\u001b[0m         write\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m write, (_, _, skip_none) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(values, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrites)\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m skip_none \u001b[38;5;129;01mor\u001b[39;00m write[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     ]\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_write(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mdict\u001b[39m(values))\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/utils.py:49\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config)\u001b[0m\n\u001b[1;32m     45\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_with_config(\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc, \u001b[38;5;28minput\u001b[39m, merge_configs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig, config), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkwargs\n\u001b[1;32m     47\u001b[0m     )\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 49\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmerge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m~/miniconda3/envs/langgraph/lib/python3.11/site-packages/langgraph/graph/state.py:131\u001b[0m, in \u001b[0;36mCompiledStateGraph.attach_node.<locals>._get_state_key\u001b[0;34m(input, config, key)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SKIP_WRITE\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidUpdateError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected dict, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mget(key, SKIP_WRITE)\n",
      "\u001b[0;31mInvalidUpdateError\u001b[0m: Expected dict, got return_values={'output': ''} log=''"
     ]
    }
   ],
   "source": [
    "# Run the graph\n",
    "\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "for s in graph.stream({\n",
    "    \"messages\": [HumanMessage(content=\"\"\"Search for the latest AI technology trends in 2024,\n",
    "            summarize the content. After summarise pass it on to insight researcher\n",
    "            to provide insights for each topic\"\"\")]\n",
    "}):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langgraph",
   "language": "python",
   "name": "langgraph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
