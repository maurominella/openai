{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60b13b70-83c3-4f35-b8f8-c0b18a0134b6",
   "metadata": {},
   "source": [
    "# Constants and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af4e67f1-5f84-4524-a283-cd7c4e8a2ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv # requires python-dotenv\n",
    "\n",
    "load_dotenv(\"./../../config/credentials_my.env\")\n",
    "QUESTION = \"Create a visualization of a sinewave\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09853b2c-7409-4e6f-a405-9fbb13cc4e5a",
   "metadata": {},
   "source": [
    "# Create a client to connecto Azure OpenAI service and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f982b2-460e-4342-a1b4-68575d34236a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client.base_url: https://mmoaiswc-01.openai.azure.com/openai/\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "# Create the client\n",
    "client = AzureOpenAI(\n",
    "    # api_key        = os.getenv(\"AZURE_OPENAI_API_KEY\"),  \n",
    "    # api_version    = os.getenv(\"AZURE_OPENAI_API_VERSION\"), # at least 2024-02-15-preview\n",
    "    # azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "print(f\"client.base_url: {client.base_url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b52996-d4b6-4a73-a1df-56a53af0f3a3",
   "metadata": {},
   "source": [
    "# Check all files already uploaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6daee676-d6d5-4cfd-b184-61d4d8120f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_uploaded_files (client:AzureOpenAI, delete:bool=False):\n",
    "    i = 0\n",
    "    for file in client.files.list().data:\n",
    "        i += 1\n",
    "        if delete:\n",
    "            print(f\"File {i}: {file.filename} (id={file.id}) is being deleted...\")\n",
    "            client.files.delete(file.id) # un-comment this line if you want to delete it\n",
    "        else:\n",
    "            print(f\"File {i}: {file.filename} (id={file.id})\")\n",
    "\n",
    "list_uploaded_files(client=client, delete=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce47dca-dbec-44b7-ad58-4cd6b6853d31",
   "metadata": {},
   "source": [
    "# Upload the Assistant file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a83cbae6-5317-4f9b-8f5b-9858f4001e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading ./data/turbines.csv...\n",
      "file_ids: ['assistant-OPjYJtts8An7vbZntAZ5X2aS']\n",
      "File 1: turbines.csv (id=assistant-OPjYJtts8An7vbZntAZ5X2aS)\n"
     ]
    }
   ],
   "source": [
    "from openai.types import FileObject\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_FOLDER = \"./data/\"\n",
    "\n",
    "def upload_file(client: AzureOpenAI, path: str) -> FileObject:\n",
    "    with Path(path).open(\"rb\") as f:\n",
    "        return client.files.create(file=f, purpose=\"assistants\")\n",
    "\n",
    "arr = os.listdir(DATA_FOLDER)\n",
    "assistant_files = []\n",
    "for file in arr:\n",
    "    filePath = DATA_FOLDER + file\n",
    "    print(f\"Uploading {filePath}...\")\n",
    "    assistant_files.append(upload_file(client, filePath))\n",
    "\n",
    "file_ids = [file.id for file in assistant_files]\n",
    "\n",
    "print(f\"file_ids: {file_ids}\")\n",
    "\n",
    "list_uploaded_files(client=client, delete=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99790e83-6846-4697-a740-f861ebe0ad58",
   "metadata": {},
   "source": [
    "# Create an assistant with `code_interpreter` and `file_ids`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3959614c-ca3b-42f8-bb56-b2bc53e661d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Assistant(id='asst_amXPIAkn9fE9fq9vcFS9aiTq', created_at=1737888711, description='You are a helpful AI assistant who helps answering questions', instructions=\"\\n        You are an assistant that can help manage wind turbine farm.\\n        The turbines operating ranges are output voltages of 33kv-35kv and RPM of 15-25. Wind speed is measured in miles per hour.\\n        Maintenance should occur every 12 months. Greet the user by saying, 'Welcome Turbine Management Assistant.\\n        \", metadata={}, model='gpt-4o', name='Smart Assistant', object='assistant', tools=[CodeInterpreterTool(type='code_interpreter')], response_format='auto', temperature=1.0, tool_resources=ToolResources(code_interpreter=ToolResourcesCodeInterpreter(file_ids=['assistant-OPjYJtts8An7vbZntAZ5X2aS']), file_search=None), top_p=1.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assistant = client.beta.assistants.create(\n",
    "    name=\"Smart Assistant\",\n",
    "    description=\"You are a helpful AI assistant who helps answering questions\",\n",
    "    instructions = \"\"\"\n",
    "        You are an assistant that can help manage wind turbine farm.\n",
    "        The turbines operating ranges are output voltages of 33kv-35kv and RPM of 15-25. Wind speed is measured in miles per hour.\n",
    "        Maintenance should occur every 12 months. Greet the user by saying, 'Welcome Turbine Management Assistant.\n",
    "        \"\"\",\n",
    "    model = os.getenv(\"AZURE_OPENAI_CHAT_DEPLOYMENT_NAME\"),\n",
    "    tools = [{\"type\": \"code_interpreter\"}],\n",
    "    tool_resources = {\n",
    "        \"code_interpreter\": {\"file_ids\": file_ids}\n",
    "    }\n",
    ")\n",
    "\n",
    "assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fa4a89-5b46-4827-886d-e9c4de63d6db",
   "metadata": {},
   "source": [
    "# Create a conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882859a2-5fff-48e2-a0ad-660cf9114563",
   "metadata": {},
   "source": [
    "## First, create a thread...\n",
    "Note that `code_interpreter` and `file_search` are empty, because they are associated with the `assistant`, not to the `thread`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "315555f6-73bf-4354-ab2e-8fde04c6bef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread(id='thread_TA0bqpMg9QuiLoj4tSCrwy3k', created_at=1737888712, metadata={}, object='thread', tool_resources=ToolResources(code_interpreter=None, file_search=None))\n"
     ]
    }
   ],
   "source": [
    "thread = client.beta.threads.create()\n",
    "print(thread)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959fd89f-120a-463c-89c4-73354ae86e41",
   "metadata": {},
   "source": [
    "## ...then, add a message to the thread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca3e1a29-4bd0-4dd3-8c36-47545c4fea20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user question to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"What is the status of turbine 1001 and 1003\"\n",
    ")\n",
    "\n",
    "thread_messages = client.beta.threads.messages.list(thread.id)\n",
    "\n",
    "# print(thread_messages.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056efd30-c771-41d8-9b26-2c0119f22a1a",
   "metadata": {},
   "source": [
    "# Create a Run and check its status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "900ab038-56dd-49f8-ba60-1b7fde34a3cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run status: queued\n",
      "Final run status: completed\n",
      "\n",
      "\n",
      ">>> MESSAGE 1:\n",
      "What is the status of turbine 1001 and 1003\n",
      "\n",
      "\n",
      ">>> MESSAGE 2:\n",
      "Welcome Turbine Management Assistant. Please allow me a moment to analyze the uploaded file and retrieve the status of turbines 1001 and 1003.\n",
      "\n",
      "\n",
      ">>> MESSAGE 3:\n",
      "The status of the turbines are as follows:\n",
      "\n",
      "- **Turbine 1001:**\n",
      "  - Wind Speed: 32 mph\n",
      "  - RPM: 18\n",
      "  - Voltage: 33 kV\n",
      "  - Last Maintenance Date: 2023-10-12\n",
      "\n",
      "- **Turbine 1003:**\n",
      "  - Wind Speed: 30 mph\n",
      "  - RPM: 16\n",
      "  - Voltage: 32 kV\n",
      "  - Last Maintenance Date: 2023-11-30\n",
      "\n",
      "Please note that Turbine 1003's voltage is slightly below the optimal operating range.\n",
      "\n",
      "\n",
      ">>> MESSAGE 4:\n",
      "Show me the code you used to provide the previous answer\n",
      "\n",
      "\n",
      ">>> MESSAGE 5:\n",
      "Certainly! Here's the code I used to filter and retrieve the status of turbines 1001 and 1003:\n",
      "\n",
      "```python\n",
      "# Filter the data for turbines 1001 and 1003\n",
      "turbine_status = data[data['Turbine_ID'].isin([1001, 1003])]\n",
      "\n",
      "# Display the filtered data\n",
      "turbine_status\n",
      "```\n",
      "\n",
      "This code uses the pandas library to filter the rows in the dataset that correspond to turbines with IDs 1001 and 1003, and then displays their information.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time, json\n",
    "\n",
    "run              = client.beta.threads.runs.create(\n",
    "    thread_id    = thread.id,\n",
    "    assistant_id = assistant.id,\n",
    ")\n",
    "\n",
    "while client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id).status in [\"queued\", \"in_progress\"]:\n",
    "    print(f\"Run status: {run.status}\")\n",
    "    time.sleep(5)\n",
    "\n",
    "print(f\"Final run status: {client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id).status}\\n\\n\")\n",
    "\n",
    "# extract the result from the run, if successfull\n",
    "for m in enumerate(reversed(client.beta.threads.messages.list(thread_id=thread.id).data),1):\n",
    "    print(f\">>> MESSAGE {m[0]}:\\n{m[1].content[0].text.value}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfe9ef2-0a43-4232-88f8-4f646c490469",
   "metadata": {},
   "source": [
    "# Folow-up question below, then run the cell above to calculate and show it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a05f7e3b-a46b-401e-8ca6-ccaf9e849324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a user question to the thread\n",
    "message = client.beta.threads.messages.create(\n",
    "    thread_id=thread.id,\n",
    "    role=\"user\",\n",
    "    content=\"Show me the code you used to provide the previous answer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3aa366-c518-422f-b48d-b7252990d3c7",
   "metadata": {},
   "source": [
    "# START teardown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "016bf385-710d-4099-b4dc-fc869e5f8648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 files to delete\n",
      "File 1/1: turbines.csv (id=assistant-OPjYJtts8An7vbZntAZ5X2aS) is being deleted...\n"
     ]
    }
   ],
   "source": [
    "# delete files\n",
    "\n",
    "files_list = client.files.list().data\n",
    "print(f\"There are {len(files_list)} files to delete\")\n",
    "\n",
    "i = 0\n",
    "for file in files_list:\n",
    "    i += 1\n",
    "    print(f\"File {i}/{len(files_list)}: {file.filename} (id={file.id}) is being deleted...\")\n",
    "    client.files.delete(file.id) # un-comment this line if you want to delete it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54ffa831-6661-4c7c-9669-ff0f331c1e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleting thread thread_TA0bqpMg9QuiLoj4tSCrwy3k...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ThreadDeleted(id='thread_TA0bqpMg9QuiLoj4tSCrwy3k', deleted=True, object='thread.deleted')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Deleting thread {thread.id}...\")\n",
    "client.beta.threads.delete(thread.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11746c7-990a-42fb-9c74-70a192802288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 assistants to delete\n",
      "Assistant 1/1: Assistant asst_amXPIAkn9fE9fq9vcFS9aiTq) is being deleted...\n"
     ]
    }
   ],
   "source": [
    "# delete assistants\n",
    "\n",
    "assistants_list = client.beta.assistants.list().data\n",
    "print(f\"There are {len(assistants_list)} assistants to delete\")\n",
    "\n",
    "i = 0\n",
    "for assistant in assistants_list:\n",
    "    i += 1\n",
    "    print(f\"Assistant {i}/{len(assistants_list)}: Assistant {assistant.id}) is being deleted...\")\n",
    "    client.beta.assistants.delete(assistant_id=assistant.id) # un-comment this line if you want to delete it"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openai",
   "language": "python",
   "name": "openai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
