{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d0c306-0d4f-46cf-97ed-cc9bcd04419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORTED TEXT_FORMATS: ['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# Libraries, Variables and Constants\n",
    "import autogen, os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Accepted file formats for that can be stored in a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS # requires pip install markdownify\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "\n",
    "cache_seed = 41 # default seed is 41\n",
    "\n",
    "print (f\"SUPPORTED TEXT_FORMATS: {TEXT_FORMATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1a8783-225e-4eb4-a5d3-5952da7dc102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache_seed': 41,\n",
       " 'config_list': [{'model': 'gpt4-0125preview-128k',\n",
       "   'api_key': '5948e5b2b4a146cba9940adb3308d731',\n",
       "   'base_url': 'https://mmopenaiscus.openai.azure.com/',\n",
       "   'api_type': 'azure',\n",
       "   'api_version': '2024-02-15-preview'}],\n",
       " 'temperature': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_list.json',\n",
    "    filter_dict={ \"model\": {\"gpt4-0125preview-128k\"} }\n",
    ")\n",
    "\n",
    "llm_config={\n",
    "        \"cache_seed\": cache_seed,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling        \n",
    "    }\n",
    "\n",
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad62c9d6-3dc7-458c-b2d0-15a5479dedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import agents\n",
    "# you need to run pip install \"pyautogen[retrievechat]\" before running this cell, because retrievechat is\n",
    "# an optional feature of the pyautogen package that requires additional dependencies not included in the base package\n",
    "\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent # requires pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0773d2-34aa-4092-a9b7-093ce7e7fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.contrib.retrieve_assistant_agent.RetrieveAssistantAgent at 0x7fd3df65bb10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an 'RetrieveAssistantAgent' instance\n",
    "\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ace22c-08ab-41d7-b9e9-c1676f1dd921",
   "metadata": {},
   "source": [
    "# Create \"ragproxyagent\"\n",
    "By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "`docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default,\n",
    "it is set to None, which works only if the collection is already created.\n",
    "`task` indicates the kind of task we're working on. In this example, it's a `qa` task.\n",
    "`chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "`custom_text_types` is a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`.\n",
    "This only applies to files under the directories in `docs_path`. Explicitly included files and urls will be chunked regardless of their types.\n",
    "In this example, we set it to [\"mdx\"] to only process markdown files. Since no mdx files are included in the `websit/docs`,\n",
    "no files there will be processed. However, the explicitly included urls will still be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f505f2e5-7a88-4e15-a786-162a66bcf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            \"https://drlee.io/harnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            # \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            # os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": TEXT_FORMATS, # [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": \"gpt-4\", # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"mauromi_collection_042\",\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"get_or_create\": False,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "ragproxyagent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169b49e-e185-49b0-959d-8cf9391ba275",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "**Note**: if `sentence_transformers` python package is not installed. Please install it with `pip install sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cbf1d2-087f-47b0-94c6-f13f6de54c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# given a problem, we use the ragproxyagent to generate a prompt to be sent to the assistant as the initial message.\n",
    "# the assistant receives the message and generates a response. The response will be sent back to the ragproxyagent for processing.\n",
    "# The conversation continues until the termination condition is met, in RetrieveChat, the termination condition when no human-in-loop is no code block detected.\n",
    "# With human-in-loop, the conversation will continue until the user says \"exit\".\n",
    "question = \"What is autogen?\" # \"what are the limitations?\" #  \"whom is autogen particularly useful for?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, \n",
    "    message=ragproxyagent.message_generator, \n",
    "    problem=question,\n",
    "    search_string=\"autogen\" # used as an extra filter for the embeddings search: in this case, we only want to search documents that contain \"autogen\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be0f6c-90a6-49e2-ad8b-9e49ad6cb3a4",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent):\n",
    "\n",
    "AutoGen is a framework for developing LLM applications using multi-agent conversations, enabling complex task solving with minimal effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a79c69-42f0-4e2e-943f-c59b9b5ae0a9",
   "metadata": {},
   "source": [
    "## Ask the same question using normal UserProxyAgent / AssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cb378-c080-4b7b-a377-b1d8138ed948",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 1,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Autogen?\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab0a9a-e7c3-49a6-ab45-cc55cc980d20",
   "metadata": {},
   "source": [
    "### assistant (to user_proxy):\n",
    "\n",
    "\"Autogen\" can refer to different concepts depending on the context in which it's used. Here are a few possibilities:\n",
    "\n",
    "1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or scripts that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, client libraries, or even documentation from a predefined template or source code annotations. This automation can significantly speed up development processes and ensure consistency across large projects.\n",
    "\n",
    "2. **Medical or Biological Term**: The term \"autogenic\" or \"autogenesis\" can refer to processes or responses that originate from within an organism, cell, or system. For example, autogenic training is a relaxation technique developed by German psychiatrist Johannes Heinrich Schultz, which involves self-induced suggestive relaxation and stress reduction.\n",
    "\n",
    "3. **Specific Software or Libraries**: There are specific tools or libraries named \"Autogen\" designed for various purposes, such as generating code from other specifications. For example, in the context of GNU/Linux systems, there is a tool called \"AutoGen\" that is used to simplify the creation and maintenance of programs that contain large amounts of repetitive text.\n",
    "\n",
    "Without more context, it's challenging to provide a precise definition of \"Autogen.\" If you have a specific context in mind, please provide more details for a more accurate explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7d9fc-5b34-4375-a8b3-7f79d4832c69",
   "metadata": {},
   "source": [
    "## Customize Embedding Function with Azure text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0b833-df4b-4353-acda-a53b8cf45440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "azure_openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    model_name=os.environ[\"EMBEDDING_ADA_003_LARGE\"],\n",
    "    api_base=os.environ[\"AZURE_OPENAI_ENDPOINT_CA\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_CA\"],\n",
    "    api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e404270-87be-415d-a946-f3d210d7a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            \"https://drlee.io/harnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            # \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            # os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": TEXT_FORMATS, # [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": \"gpt-4\", # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"mauromi_collection_043\",\n",
    "        \"embedding_function\": azure_openai_ef,\n",
    "        \"get_or_create\": False,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "ragproxyagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba57535-119e-4d72-b072-a73ad72f4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the enhanced LLM inference features?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, message=ragproxyagent.message_generator,\n",
    "    problem=question,\n",
    "    search_string=\"autogen\" # used as an extra filter for the embeddings search: in this case, we only want to search documents that contain \"autogen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfc340-1d5e-4a5a-82cf-c8b93fa6c444",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent)\n",
    "\n",
    "Caching, error handling, multi-config inference, templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88463cd7-7194-48ce-88c4-e16c1f0863fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What Python versions are supported by Autogen?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, message=ragproxyagent.message_generator,\n",
    "    problem=question,\n",
    "    search_string=\"autogen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47975832-5696-45bf-adf6-5b6dfb3b8c14",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent):\n",
    "\n",
    "3.8 | 3.9 | 3.10 | 3.11 | 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9abda-4599-4f32-9a53-dd8cbf997604",
   "metadata": {},
   "source": [
    "# Check the cache DB with SQL Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a5ca8-4ab1-4288-8ac3-40ec35724aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the content of the cache\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(f'./.cache/{cache_seed}/cache.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List all tables in the database\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Tables in this DB: {cur.fetchall()}\\n\")\n",
    "\n",
    "# Execute a query\n",
    "cur.execute(\"SELECT * FROM Cache\")\n",
    "\n",
    "i = 1\n",
    "# Fetch and print the results\n",
    "for row in cur.fetchall():\n",
    "    print(f\"\\n\\n\\n++++++++++++++++ ROW {i} ++++++++++++++++++\\n\\n\\n\")    \n",
    "    print(row)\n",
    "    i=i+1\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fb406-3ab9-4c16-8d35-f8d169b81d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 1,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Autogen?\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dc4f3-b9c5-4a41-896b-2804ad1e23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7e817c-1b35-4ff7-b4e8-55ab655282e9",
   "metadata": {},
   "source": [
    "### assistant (to user_proxy):\n",
    "\n",
    "\"Autogen\" can refer to different concepts depending on the context in which it's used. Here are a few possibilities:\n",
    "\n",
    "1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or processes that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, client libraries, or even documentation from a set of definitions or templates. This automation can significantly speed up development and ensure consistency across different parts of a project.\n",
    "\n",
    "2. **Autogenic Training**: In a completely different context, \"Autogen\" could be a shorthand or misspelling for \"autogenic training,\" which is a relaxation technique developed by the German psychiatrist Johannes Heinrich Schultz. Autogenic training involves a series of exercises designed to induce a state of relaxation and reduce stress through self-suggestion and focusing on bodily sensations.\n",
    "\n",
    "3. **Other Contexts**: The term could also be part of a brand name, product, or specific technology in various industries, such as automotive, aerospace, or health and wellness. Without more specific context, it's challenging to pinpoint exactly what \"Autogen\" refers to.\n",
    "\n",
    "If you have a specific context in mind for \"Autogen,\" providing more details could help in giving a more accurate and targeted explanation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
