{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5d71f17c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'api_type': 'open_ai',\n",
       "  'api_base': 'http://localhost:1234/v1/chat/completions',\n",
       "  'api_key': 'sk-XoMik5CKunt0hvEJH5QgT3B1bkFJeoQoffxGuvWBf3G3EFKn'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import autogen, os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "\n",
    "cache_seed = 42 # default seed is 41\n",
    "\n",
    "\n",
    "my_models = [\n",
    "    {\n",
    "        'model': os.environ['GPT4-1106-128k'],\n",
    "        'api_key': os.environ['AZURE_OPENAI_API_KEY'],\n",
    "        \"base_url\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "        \"api_type\": os.environ['OPENAI_API_TYPE'],\n",
    "        \"api_version\": os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    },\n",
    "    {\n",
    "        'model': os.environ['GPT432-0613-32k'],\n",
    "        'api_key': os.environ['AZURE_OPENAI_API_KEY'],\n",
    "        \"base_url\": os.environ['AZURE_OPENAI_ENDPOINT'],\n",
    "        \"api_type\": os.environ['OPENAI_API_TYPE'],\n",
    "        \"api_version\": os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    },\n",
    "    {\n",
    "        'model': os.environ['GPT4-0125PREVIEW-128k'],\n",
    "        'api_key': os.environ['AZURE_OPENAI_API_KEY_SCUS'],\n",
    "        \"base_url\": os.environ['AZURE_OPENAI_ENDPOINT_SCUS'],\n",
    "        \"api_type\": os.environ['OPENAI_API_TYPE'],\n",
    "        \"api_version\": os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    },\n",
    "    {\n",
    "        \"api_type\": \"open_ai\",\n",
    "        \"api_base\": \"http://localhost:1234/v1/chat/completions\",\n",
    "        \"api_key\":  \"sk-XoMik5CKunt0hvEJH5QgT3B1bkFJeoQoffxGuvWBf3G3EFKn\"\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "# need for converting to string\n",
    "os.environ['models_var'] = json.dumps(my_models)\n",
    "\n",
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_var',\n",
    "    filter_dict={\n",
    "        \"api_base\": {\n",
    "            \"http://localhost:1234/v1/chat/completions\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8b9e6b3f-db1c-4f06-b821-2bc2e7176417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'request_timeout': 600,\n",
       " 'seed': 42,\n",
       " 'config_list': [{'api_type': 'open_ai',\n",
       "   'api_base': 'http://localhost:1234/v1/chat/completions',\n",
       "   'api_key': 'sk-XoMik5CKunt0hvEJH5QgT3B1bkFJeoQoffxGuvWBf3G3EFKn'}],\n",
       " 'temperature': 0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_config={\n",
    "    \"request_timeout\": 600,\n",
    "    \"seed\": cache_seed,  # seed for caching and reproducibility\n",
    "    \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "    \"temperature\": 0,  # temperature for sampling\n",
    "}\n",
    "\n",
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "89049d9f-3799-4bd8-89d4-b175a385755d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.assistant_agent.AssistantAgent at 0x7fcfe984a690>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config=llm_config,\n",
    "    system_message = \"You are a coder specializing in Python.\"\n",
    ")\n",
    "\n",
    "assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14c6c8c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.user_proxy_agent.UserProxyAgent at 0x7fcfe9848e90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 10,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    }\n",
    ")\n",
    "\n",
    "user_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00bf4dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[autogen.oai.client: 03-29 18:58:36] {122} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.assistant_agent.AssistantAgent at 0x7fcfe9833e30>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    llm_config={\n",
    "        \"cache_seed\": cache_seed,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0,  # temperature for sampling\n",
    "    },  # configuration for autogen's enhanced inference API which is compatible with OpenAI API\n",
    ")\n",
    "\n",
    "assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"No default IOStream has been set, defaulting to IOConsole\")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n",
    "    summary_method=\"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3b143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# autogen.Completion.clear_cache(seed=cache_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fda647-042b-454a-ad78-bf921c63857a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "for cr in chat_res.chat_history:\n",
    "    print(f\"Step {i} --> {cr[\"content\"].rstrip()[-9:]}\")\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728d55c9-2180-4148-8278-0b4d411eef0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the content of the cache\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(f'./.cache/{cache_seed}/cache.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List all tables in the database\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Tables in this DB: {cur.fetchall()}\\n\")\n",
    "\n",
    "# Execute a query\n",
    "cur.execute(\"SELECT * FROM Cache\")\n",
    "\n",
    "# Fetch and print the results\n",
    "for row in cur.fetchall():\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6215b3-9bb9-4db9-81fb-bfae1a97ae14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# followup of the previous question\n",
    "user_proxy.send(\n",
    "    recipient=assistant,\n",
    "    message=\"\"\"Plot a chart of their stock price change YTD. Save the data to stock_price_ytd.csv, and save the plot to stock_price_ytd.png.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b49710-fbb7-4196-9392-fdfdc7877d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Image\n",
    "\n",
    "try:\n",
    "    image = Image(filename=\"coding/stock_price_ytd.png\")\n",
    "    display(image)\n",
    "except FileNotFoundError:\n",
    "    print(\"Image not found. Please check the file name and modify if necessary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a365d4-6110-4edd-8e71-5417a29875da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
