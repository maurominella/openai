{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04394246-62aa-4148-b653-73f1244c0886",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries and Constants\n",
    "\n",
    "SERVER_IP = '192.168.27.99'\n",
    "API_KEY   = 'whatever'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceacc4ac-6e4b-4ef7-ad38-3e9577129f21",
   "metadata": {},
   "source": [
    "# Using Mistral with straight OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ee3ea6-5f63-435d-ba6f-b79b6636d96d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletionMessage(content=\" Here's the Python code to list all numbers between 110 and 120:\\n```python\\nfor num in range(110, 121):\\n    print(num)\\n```\", role='assistant', function_call=None, tool_calls=None)\n"
     ]
    }
   ],
   "source": [
    "# Example: reuse your existing OpenAI setup\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=F\"http://{SERVER_IP}:1234/v1\", api_key=API_KEY)\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful. You are also a Python expert.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me some Python code that lists all numbers between 110 and 120.\"}\n",
    "  ],\n",
    "  temperature=0.7,\n",
    ")\n",
    "\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c20abc0-65fa-40cd-b4e8-73b8fc06ebc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello! I'm an AI assistant designed to help you with your queries and tasks. How can I assist you today?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      ">  Tell me some Python code that lists all numbers between 110 and 120.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Here is a simple Python code that will list all the numbers between 110 and 120:\n",
      "```\n",
      "for num in range(110, 121):\n",
      "    print(num)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Chat with an intelligent assistant in your terminal\n",
    "from openai import OpenAI\n",
    "\n",
    "# Point to the local server\n",
    "client = OpenAI(base_url=F\"http://{SERVER_IP}:1234/v1\", api_key=API_KEY)\n",
    "\n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello, introduce yourself to someone opening this program for the first time. Be concise.\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\",\n",
    "        messages=history,\n",
    "        temperature=0.7,\n",
    "        stream=True,\n",
    "    )\n",
    "\n",
    "    new_message = {\"role\": \"assistant\", \"content\": \"\"}\n",
    "    \n",
    "    for chunk in completion:\n",
    "        if chunk.choices[0].delta.content:\n",
    "            print(chunk.choices[0].delta.content, end=\"\", flush=True)\n",
    "            new_message[\"content\"] += chunk.choices[0].delta.content\n",
    "\n",
    "    history.append(new_message)\n",
    "    \n",
    "    # Uncomment to see chat history\n",
    "    # import json\n",
    "    # gray_color = \"\\033[90m\"\n",
    "    # reset_color = \"\\033[0m\"\n",
    "    # print(f\"{gray_color}\\n{'-'*20} History dump {'-'*20}\\n\")\n",
    "    # print(json.dumps(history, indent=2))\n",
    "    # print(f\"\\n{'-'*55}\\n{reset_color}\")\n",
    "\n",
    "    print()\n",
    "    history.append({\"role\": \"user\", \"content\": input(\"> \")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29a21c7-e1e0-45ce-8e73-f02e296fbffb",
   "metadata": {},
   "source": [
    "# Using Mistral with Autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d71f17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen, os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "\n",
    "cache_seed = 42 # default seed is 41\n",
    "\n",
    "my_models = [\n",
    "    # mistral-7b-instruct-v0.1.Q6_K.gguf\n",
    "    {\n",
    "        \"model\": \"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q5_0.gguf\",\n",
    "        \"api_type\": \"open_ai\",\n",
    "        \"api_base\": F\"http://{SERVER_IP}:1234/v1\",\n",
    "        \"api_key\":  API_KEY # \"sk-XoMik5CKunt0hvEJH5QgT3B1bkFJeoQoffxGuvWBf3G3EFKn\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# need for converting to string\n",
    "os.environ['models_var'] = json.dumps(my_models)\n",
    "\n",
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_var',\n",
    "    filter_dict={\n",
    "        \"api_base\": {\n",
    "           f\"http://{SERVER_IP}:1234/v1\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "config_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e6b3f-db1c-4f06-b821-2bc2e7176417",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config={\n",
    "    # \"request_timeout\": 600,\n",
    "    \"seed\": cache_seed,  # seed for caching and reproducibility\n",
    "    \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "    \"temperature\": 0  # temperature for sampling\n",
    "}\n",
    "\n",
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6c8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 10,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg=lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    system_message = \"\"\"Reply TERMINATE if the task has been completed\"\"\",\n",
    "    \n",
    "    llm_config = llm_config,\n",
    ")\n",
    "\n",
    "user_proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89049d9f-3799-4bd8-89d4-b175a385755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "import autogen\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message = \"You are a coder specializing in Python.\",\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d7fb0e1-5a7e-4048-9a0d-dd9082f27194",
   "metadata": {},
   "outputs": [],
   "source": [
    "task = \"\"\"\n",
    "write a python method to output number 101 to 106\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144644e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# the assistant receives a message from the user_proxy, which contains the task description\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=task,\n",
    "    model=\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF/mistral-7b-instruct-v0.1.Q6_K.gguf\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an intelligent assistant. You always provide well-reasoned answers that are both correct and helpful. You are also a Python expert.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Tell me some Python code that lists all numbers between 110 and 120.\"}\n",
    "    ],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
