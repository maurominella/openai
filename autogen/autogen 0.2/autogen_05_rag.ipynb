{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d0c306-0d4f-46cf-97ed-cc9bcd04419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORTED TEXT_FORMATS: ['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# Libraries, Variables and Constants\n",
    "import autogen, os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Accepted file formats for that can be stored in a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS # requires pip install markdownify\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "\n",
    "cache_seed = 41 # default seed is 41\n",
    "\n",
    "print (f\"SUPPORTED TEXT_FORMATS: {TEXT_FORMATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1a8783-225e-4eb4-a5d3-5952da7dc102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_list.json',\n",
    "    filter_dict={ \"model\": {\"gpt-4o-2024-05-13\"} }\n",
    ")\n",
    "\n",
    "llm_config={\n",
    "        \"cache_seed\": cache_seed,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling        \n",
    "    }\n",
    "\n",
    "# llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad62c9d6-3dc7-458c-b2d0-15a5479dedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import agents\n",
    "# you need to run pip install \"ag2[retrievechat]\" before running this cell, because retrievechat is\n",
    "# an optional feature of the ag2 package that requires additional dependencies not included in the base package\n",
    "\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent # requires pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0773d2-34aa-4092-a9b7-093ce7e7fa6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an 'RetrieveAssistantAgent' instance\n",
    "\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ace22c-08ab-41d7-b9e9-c1676f1dd921",
   "metadata": {},
   "source": [
    "# Create \"ragproxyagent\"\n",
    "By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "`docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default,\n",
    "it is set to None, which works only if the collection is already created.\n",
    "`task` indicates the kind of task we're working on. In this example, it's a `qa` task.\n",
    "`chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "`custom_text_types` is a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`.\n",
    "This only applies to files under the directories in `docs_path`. Explicitly included files and urls will be chunked regardless of their types.\n",
    "In this example, we set it to [\"mdx\"] to only process markdown files. Since no mdx files are included in the `websit/docs`,\n",
    "no files there will be processed. However, the explicitly included urls will still be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f505f2e5-7a88-4e15-a786-162a66bcf84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            \"https://drlee.io/harnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": TEXT_FORMATS, # [\"mdx\"],TEXT_FORMATS\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": \"gpt-4\", # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": f\"mauromi_collection_{cache_seed}\",\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"get_or_create\": False,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169b49e-e185-49b0-959d-8cf9391ba275",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "**Note**: if `sentence_transformers` python package is not installed. Please install it with `pip install sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cbf1d2-087f-47b0-94c6-f13f6de54c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "File /home/mauromi/git_repos/openai/autogen/../website/docs does not exist. Skipping.\n",
      "2024-06-02 01:35:06,160 - autogen.agentchat.contrib.retrieve_user_proxy_agent - INFO - Found 4 chunks.\u001b[0m\n",
      "2024-06-02 01:35:06,162 - autogen.agentchat.contrib.vectordb.chromadb - INFO - No content embedding is provided. Will use the VectorDB's embedding function to generate the content embedding.\u001b[0m\n",
      "Number of requested results 20 is greater than number of elements in index 4, updating n_results = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VectorDB returns doc_ids:  [['16650301', '31e7b9fd']]\n",
      "\u001b[32mAdding content of doc 16650301 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: What is autogen?\n",
      "\n",
      "Context is: # Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\n",
      "\n",
      "Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\n",
      "================================================================================================\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "·[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\n",
      "\n",
      "3 min read·Dec 19, 2023--\n",
      "\n",
      "Listen\n",
      "\n",
      "Share\n",
      "\n",
      "![]()Introduction\n",
      "============\n",
      "\n",
      "In the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI’s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\n",
      "\n",
      "Step 1: Setting Up Your Environment\n",
      "===================================\n",
      "\n",
      "Before diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\n",
      "\n",
      "```\n",
      "!pip install ag2==0.2.0b5\n",
      "```\n",
      "Step 2: Importing the Required Modules\n",
      "======================================\n",
      "\n",
      "Once the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\n",
      "\n",
      "```\n",
      "from autogen import config_list_from_json  \n",
      "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \n",
      "from autogen import UserProxyAgent  \n",
      "  \n",
      "# config_list = config_list_from_json(\"OAI_CONFIG_LIST\")  \n",
      "  \n",
      "config_list = [  \n",
      "    {  \n",
      "        'model': 'gpt-4',  \n",
      "        'api_key': '<put your openai key here>',  \n",
      "    },  \n",
      "]\n",
      "```\n",
      "Step 3: Creating the GPTAssistantAgent\n",
      "======================================\n",
      "\n",
      "The next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None  \n",
      "    })\n",
      "```\n",
      "Step 4: Setting Up the UserProxyAgent\n",
      "=====================================\n",
      "\n",
      "The `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\n",
      "\n",
      "```\n",
      "user_proxy = UserProxyAgent(  \n",
      "    name=\"user_proxy\",  \n",
      "    code_execution_config={  \n",
      "        \"work_dir\": \"coding\"  \n",
      "    },  \n",
      "    human_input_mode=\"NEVER\")\n",
      "```\n",
      "Step 5: Initiating the Task\n",
      "===========================\n",
      "\n",
      "With both agents configured, you can now initiate a task. For example, to print “Hello World”, you would do the following:\n",
      "\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")\n",
      "```\n",
      "Step 6: Enabling Advanced Features\n",
      "==================================\n",
      "\n",
      "For more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None,  \n",
      "        \"tools\": [{\"type\": \"code_interpreter\"}],  \n",
      "    })\n",
      "```\n",
      "then test it with this:\n",
      "\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Get the price of gold and print a visual of the last 4 days closing price\")\n",
      "```\n",
      "![]()Conclusion\n",
      "==========\n",
      "\n",
      "The integration of AutoGen and OpenAI’s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\n",
      "\n",
      "Future Perspectives\n",
      "===================\n",
      "\n",
      "While there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\n",
      "\n",
      "[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\n",
      "\n",
      "--\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\n",
      "--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.8K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\n",
      "\n",
      "[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Press](mailto:pressinquiries@medium.com?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "AutoGen is a tool that integrates with OpenAI's GPTs to create advanced, multi-agent systems for complex coding tasks.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# given a problem, we use the ragproxyagent to generate a prompt to be sent to the assistant as the initial message.\n",
    "# the assistant receives the message and generates a response. The response will be sent back to the ragproxyagent for processing.\n",
    "# The conversation continues until the termination condition is met, in RetrieveChat, the termination condition when no human-in-loop is no code block detected.\n",
    "# With human-in-loop, the conversation will continue until the user says \"exit\".\n",
    "question = \"What is autogen?\" # \"what are the limitations?\" #  \"whom is autogen particularly useful for?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, \n",
    "    message=ragproxyagent.message_generator, \n",
    "    problem=question,\n",
    "    search_string=\"autogen\" # used as an extra filter for the embeddings search: in this case, we only want to search documents that contain \"autogen\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be0f6c-90a6-49e2-ad8b-9e49ad6cb3a4",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent):\n",
    "\n",
    "AutoGen is a framework for developing LLM applications using multi-agent conversations, enabling complex task solving with minimal effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a79c69-42f0-4e2e-943f-c59b9b5ae0a9",
   "metadata": {},
   "source": [
    "## Ask the same question using normal UserProxyAgent / AssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f78cb378-c080-4b7b-a377-b1d8138ed948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is Autogen?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\"Autogen\" can refer to different things depending on the context in which it is used. Here are a few possible meanings:\n",
      "\n",
      "1. **Autogen (Software Development)**: In software development, \"autogen\" often refers to automated generation of code or configuration files. This can include tools or scripts that automatically generate boilerplate code, configuration files, or other necessary components to streamline the development process.\n",
      "\n",
      "2. **Autogen (Medical/Health)**: In the context of health and wellness, \"autogen\" might refer to autogenic training, a relaxation technique that involves self-suggestion to induce a state of relaxation and reduce stress.\n",
      "\n",
      "3. **Autogen (General)**: More generally, \"autogen\" could be shorthand for any process or system that is automatically generated or self-generating.\n",
      "\n",
      "If you have a specific context in mind, please provide more details so I can give a more precise explanation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "\"Autogen\" can refer to different things depending on the context in which it is used. Here are a few possible meanings:\n",
      "\n",
      "1. **Autogen (Software Development)**: In software development, \"autogen\" often refers to automated generation of code or configuration files. This can include tools or scripts that automatically generate boilerplate code, configuration files, or other necessary components to streamline the development process.\n",
      "\n",
      "2. **Autogen (Medical/Health)**: In the context of health and wellness, \"autogen\" might refer to autogenic training, a relaxation technique that involves self-suggestion to induce a state of relaxation and reduce stress.\n",
      "\n",
      "3. **Autogen (General)**: More generally, \"autogen\" could be shorthand for any process or system that is automatically generated or self-generating.\n",
      "\n",
      "If you have a specific context in mind, please provide more details so I can give a more precise explanation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "It looks like you've reiterated the information I provided. If you have a specific context or additional questions about \"Autogen,\" please let me know, and I'll be happy to provide more detailed information or clarification!\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 1,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Autogen?\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7d9fc-5b34-4375-a8b3-7f79d4832c69",
   "metadata": {},
   "source": [
    "## Customize Embedding Function with Azure text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd0b833-df4b-4353-acda-a53b8cf45440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "azure_openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    model_name=os.environ[\"EMBEDDING_ADA_003_LARGE\"],\n",
    "    api_base=os.environ[\"AZURE_OPENAI_ENDPOINT_CA\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_CA\"],\n",
    "    api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e404270-87be-415d-a946-f3d210d7a236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            \"https://drlee.io/harnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            # \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            # os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": TEXT_FORMATS, # [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": \"gpt-4\", # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"mauromi_collection_043\",\n",
    "        \"embedding_function\": azure_openai_ef,\n",
    "        \"get_or_create\": False,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "ragproxyagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba57535-119e-4d72-b072-a73ad72f4fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what are the enhanced LLM inference features?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, message=ragproxyagent.message_generator,\n",
    "    problem=question,\n",
    "    search_string=\"autogen\" # used as an extra filter for the embeddings search: in this case, we only want to search documents that contain \"autogen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfc340-1d5e-4a5a-82cf-c8b93fa6c444",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent)\n",
    "\n",
    "Caching, error handling, multi-config inference, templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88463cd7-7194-48ce-88c4-e16c1f0863fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What Python versions are supported by Autogen?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, message=ragproxyagent.message_generator,\n",
    "    problem=question,\n",
    "    search_string=\"autogen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47975832-5696-45bf-adf6-5b6dfb3b8c14",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent):\n",
    "\n",
    "3.8 | 3.9 | 3.10 | 3.11 | 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9abda-4599-4f32-9a53-dd8cbf997604",
   "metadata": {},
   "source": [
    "# Check the cache DB with SQL Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1a5ca8-4ab1-4288-8ac3-40ec35724aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the content of the cache\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(f'./.cache/{cache_seed}/cache.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List all tables in the database\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Tables in this DB: {cur.fetchall()}\\n\")\n",
    "\n",
    "# Execute a query\n",
    "cur.execute(\"SELECT * FROM Cache\")\n",
    "\n",
    "i = 1\n",
    "# Fetch and print the results\n",
    "for row in cur.fetchall():\n",
    "    print(f\"\\n\\n\\n++++++++++++++++ ROW {i} ++++++++++++++++++\\n\\n\\n\")    \n",
    "    print(row)\n",
    "    i=i+1\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fb406-3ab9-4c16-8d35-f8d169b81d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 1,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Autogen?\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dc4f3-b9c5-4a41-896b-2804ad1e23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7e817c-1b35-4ff7-b4e8-55ab655282e9",
   "metadata": {},
   "source": [
    "### assistant (to user_proxy):\n",
    "\n",
    "\"Autogen\" can refer to different concepts depending on the context in which it's used. Here are a few possibilities:\n",
    "\n",
    "1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or processes that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, client libraries, or even documentation from a set of definitions or templates. This automation can significantly speed up development and ensure consistency across different parts of a project.\n",
    "\n",
    "2. **Autogenic Training**: In a completely different context, \"Autogen\" could be a shorthand or misspelling for \"autogenic training,\" which is a relaxation technique developed by the German psychiatrist Johannes Heinrich Schultz. Autogenic training involves a series of exercises designed to induce a state of relaxation and reduce stress through self-suggestion and focusing on bodily sensations.\n",
    "\n",
    "3. **Other Contexts**: The term could also be part of a brand name, product, or specific technology in various industries, such as automotive, aerospace, or health and wellness. Without more specific context, it's challenging to pinpoint exactly what \"Autogen\" refers to.\n",
    "\n",
    "If you have a specific context in mind for \"Autogen,\" providing more details could help in giving a more accurate and targeted explanation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
