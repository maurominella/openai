{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321621d2-8d68-4814-93f8-58760fea3435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries, Variables and Constants\n",
    "import autogen, os\n",
    "import json\n",
    "from dotenv import load_dotenv # pip install python-dotenv\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "cache_seed = 42 # default seed is 41"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3337e1-48fc-40ba-b39a-563eaf1f0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_models = [\n",
    "    {\n",
    "        'model': os.environ['GPT-4T-20240409GA-128K'],\n",
    "        \"base_url\": os.environ['AZURE_OPENAI_ENDPOINT_SEC'],\n",
    "        'api_key': os.environ['AZURE_OPENAI_API_KEY_SEC'],\n",
    "        \"api_type\": os.environ['OPENAI_API_TYPE'],\n",
    "        \"api_version\": os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    },\n",
    "    {\n",
    "        'model': os.environ['GPT-4O-20240513-128K'],\n",
    "        'api_key': os.environ['AZURE_OPENAI_API_KEY_SCUS'],\n",
    "        \"base_url\": os.environ['AZURE_OPENAI_ENDPOINT_SCUS'],\n",
    "        \"api_type\": os.environ['OPENAI_API_TYPE'],\n",
    "        \"api_version\": os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    },\n",
    "    {\n",
    "        'model': os.environ['GPT-4T-VISIONPREVIEW-128K'],\n",
    "        \"base_url\": os.environ['AZURE_OPENAI_ENDPOINT_SEC'],\n",
    "        'api_key': os.environ['AZURE_OPENAI_API_KEY_SEC'],\n",
    "        \"api_type\": os.environ['OPENAI_API_TYPE'],\n",
    "        \"api_version\": os.environ['AZURE_OPENAI_API_VERSION']\n",
    "    }\n",
    "]\n",
    "\n",
    "# need for converting to string\n",
    "os.environ['models_var'] = json.dumps(my_models)\n",
    "\n",
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_var',\n",
    "    filter_dict={\n",
    "        \"model\": {\"gpt-4o-2024-05-13\", \"gpt-4\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# config_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916fce16-a411-4e36-8e58-98b873d862e5",
   "metadata": {},
   "source": [
    "## Load autogen configuration from external JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7381dc-d6c6-4a96-8301-bc4a1c5f5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_list.json',\n",
    "    filter_dict={\n",
    "        \"model\": {\"gpt-4o-2024-05-13\", \"gpt-4\"}\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dae4d8",
   "metadata": {},
   "source": [
    "### [Difference between cache_seed and OpenAI's seed parameter](https://microsoft.github.io/autogen/docs/topics/llm-caching/)\n",
    "OpenAI v1.1 introduced a new parameter `seed`. The difference between AutoGen's `cache_seed` and OpenAI's `seed` is AutoGen uses an explicit request cache to guarantee the exactly same output is produced for the same input and when cache is hit, no OpenAI API call will be made. OpenAI's `seed` is a best-effort deterministic sampling with no guarantee of determinism. When using OpenAI's `seed` with `cache_seed` set to `None`, even for the same input, an OpenAI API call will be made and there is no guarantee for getting exactly the same output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6fbbe-aa03-435e-8e8b-b27414c44eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "autogen.Completion.clear_cache()\n",
    "\n",
    "llm_config={\n",
    "    \"cache_seed\":  cache_seed, # None, cache_seed,  # seed for caching and reproducibility\n",
    "    \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "    \"temperature\": 0,  # temperature for sampling\n",
    "}\n",
    "\n",
    "# llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b966bd07-0341-4e78-85ef-c00f75e2d295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a UserProxyAgent instance named \"user_proxy\"\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 10,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53dd218-ae14-44c6-9e06-4cd1b56f0c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an AssistantAgent named \"assistant\"\n",
    "\n",
    "assistant = autogen.AssistantAgent(\n",
    "    name       = \"assistant\",\n",
    "    llm_config = llm_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e8ccb0-2da3-4075-a3f9-d38f1ea09c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# the assistant receives a message from the user_proxy, which contains the task description in the \"message\" field\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"\"\"What date is today? Compare the year-to-date gain for META and TESLA.\"\"\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5d079a-20dd-4cb5-ac8a-fd97258f1fce",
   "metadata": {},
   "source": [
    "## Only if we couldn't gather the stock price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a7e821-107e-4216-b946-24b22865a829",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# followup of the previous question, if needed\n",
    "user_proxy.send(\n",
    "    recipient=assistant,\n",
    "    message=\"\"\"Please try again and use another way to gather the stock price. For example, use yfinance.\"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8c6d8-6773-4ecb-9079-ac208692226b",
   "metadata": {},
   "source": [
    "## Checks the Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb94f571-35ce-493d-bfc1-1fd14e158744",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for cr in chat_res.chat_history:\n",
    "    print(f'Step {i} --> {cr[\"content\"].rstrip()[-9:]}')\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a47c96-da27-42ae-8b60-e3a0ceab518c",
   "metadata": {},
   "source": [
    "## Check the history in .cache/\\<seed\\>/cache.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50d0dea-3a66-4d2a-b07e-7518da16a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the content of the cache\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(f'./.cache/{cache_seed}/cache.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List all tables in the database\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Tables in this DB: {cur.fetchall()}\\n\")\n",
    "\n",
    "# Execute a query\n",
    "cur.execute(\"SELECT * FROM Cache\")\n",
    "\n",
    "# Fetch and print the results\n",
    "for row in cur.fetchall():\n",
    "    print(row)\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041f5a6d",
   "metadata": {},
   "source": [
    "## Ask an additional sub-question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4174a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "user_proxy.send(\n",
    "    recipient=assistant,\n",
    "    message=\"\"\"\n",
    "    Plot a chart of their stock prince change YTD. Save the data to stock_price_ytd.csv, and \n",
    "    save the plot to stock_price_ytd.png.\n",
    "    \"\"\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c55d31-f736-4992-9722-09083394fe3b",
   "metadata": {},
   "source": [
    "# Hic Sunt Leones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
