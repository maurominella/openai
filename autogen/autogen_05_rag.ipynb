{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7d0c306-0d4f-46cf-97ed-cc9bcd04419a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUPPORTED TEXT_FORMATS: ['txt', 'json', 'csv', 'tsv', 'md', 'html', 'htm', 'rtf', 'rst', 'jsonl', 'log', 'xml', 'yaml', 'yml', 'pdf']\n"
     ]
    }
   ],
   "source": [
    "# Libraries, Variables and Constants\n",
    "import autogen, os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Accepted file formats for that can be stored in a vector database instance\n",
    "from autogen.retrieve_utils import TEXT_FORMATS # requires pip install markdownify\n",
    "\n",
    "load_dotenv(\"./../credentials_my.env\")\n",
    "\n",
    "cache_seed = 41 # default seed is 41\n",
    "\n",
    "print (f\"SUPPORTED TEXT_FORMATS: {TEXT_FORMATS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1a8783-225e-4eb4-a5d3-5952da7dc102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cache_seed': 41,\n",
       " 'config_list': [{'model': 'gpt4-0125preview-128k',\n",
       "   'api_key': '5948e5b2b4a146cba9940adb3308d731',\n",
       "   'base_url': 'https://mmopenaiscus.openai.azure.com/',\n",
       "   'api_type': 'azure',\n",
       "   'api_version': '2024-02-15-preview'}],\n",
       " 'temperature': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting configurations for autogen\n",
    "config_list = autogen.config_list_from_json(\n",
    "    env_or_file='models_list.json',\n",
    "    filter_dict={ \"model\": {\"gpt4-0125preview-128k\"} }\n",
    ")\n",
    "\n",
    "llm_config={\n",
    "        \"cache_seed\": cache_seed,  # seed for caching and reproducibility\n",
    "        \"config_list\": config_list,  # a list of OpenAI API configurations\n",
    "        \"temperature\": 0  # temperature for sampling        \n",
    "    }\n",
    "\n",
    "llm_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad62c9d6-3dc7-458c-b2d0-15a5479dedc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import agents\n",
    "# you need to run pip install \"pyautogen[retrievechat]\" before running this cell, because retrievechat is\n",
    "# an optional feature of the pyautogen package that requires additional dependencies not included in the base package\n",
    "\n",
    "from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent\n",
    "from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent # requires pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a0773d2-34aa-4092-a9b7-093ce7e7fa6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.contrib.retrieve_assistant_agent.RetrieveAssistantAgent at 0x7fd3df65bb10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an 'RetrieveAssistantAgent' instance\n",
    "\n",
    "assistant = RetrieveAssistantAgent(\n",
    "    name=\"assistant\",\n",
    "    system_message=\"You are a helpful assistant.\",\n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "assistant"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ace22c-08ab-41d7-b9e9-c1676f1dd921",
   "metadata": {},
   "source": [
    "# Create \"ragproxyagent\"\n",
    "By default, the human_input_mode is \"ALWAYS\", which means the agent will ask for human input at every step. We set it to \"NEVER\" here.\n",
    "`docs_path` is the path to the docs directory. It can also be the path to a single file, or the url to a single file. By default,\n",
    "it is set to None, which works only if the collection is already created.\n",
    "`task` indicates the kind of task we're working on. In this example, it's a `qa` task.\n",
    "`chunk_token_size` is the chunk token size for the retrieve chat. By default, it is set to `max_tokens * 0.6`, here we set it to 2000.\n",
    "`custom_text_types` is a list of file types to be processed. Default is `autogen.retrieve_utils.TEXT_FORMATS`.\n",
    "This only applies to files under the directories in `docs_path`. Explicitly included files and urls will be chunked regardless of their types.\n",
    "In this example, we set it to [\"mdx\"] to only process markdown files. Since no mdx files are included in the `websit/docs`,\n",
    "no files there will be processed. However, the explicitly included urls will still be processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f505f2e5-7a88-4e15-a786-162a66bcf84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent at 0x7fd3df659710>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            \"https://drlee.io/harnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            # \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            # os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": TEXT_FORMATS, # [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": \"gpt-4\", # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"mauromi_collection_042\",\n",
    "        \"embedding_model\": \"all-mpnet-base-v2\",\n",
    "        \"get_or_create\": False,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "ragproxyagent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a169b49e-e185-49b0-959d-8cf9391ba275",
   "metadata": {},
   "source": [
    "# Example 1\n",
    "**Note**: if `sentence_transformers` python package is not installed. Please install it with `pip install sentence_transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31cbf1d2-087f-47b0-94c6-f13f6de54c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efad8561815f40d4b954aab21291d1c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e58a3e8bc32a4ead941fa55a402c5cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e990c1a899264a8ea612f9f853677db0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9558155705c94b15ac5a7e311883a040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8788a75e4c0e4a7a931de539a47a35a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "668862b9a0574202834dc7180c671ca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dfc612921946ed860b1305f63fee92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1321e2e0f5bb4701bc3146213fd0d162",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3f9a6c5e2d447fb5ead5b4fb7a53bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae4100c127ee4ce8bbe4dd70d7a2f919",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dce70e1fc94234b2aeaedf8370978f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_3', 'doc_0', 'doc_4', 'doc_1', 'doc_2']]\n",
      "\u001b[32mAdding doc_id doc_3 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_4 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: What is autogen?\n",
      "\n",
      "Context is: # Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\n",
      "\n",
      "Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\n",
      "================================================================================================\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "·[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\n",
      "\n",
      "3 min read·Dec 19, 2023--\n",
      "\n",
      "Listen\n",
      "\n",
      "Share\n",
      "\n",
      "![]()Introduction\n",
      "============\n",
      "\n",
      "In the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI’s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\n",
      "\n",
      "Step 1: Setting Up Your Environment\n",
      "===================================\n",
      "\n",
      "Before diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\n",
      "\n",
      "```\n",
      "!pip install pyautogen==0.2.0b5\n",
      "```\n",
      "Step 2: Importing the Required Modules\n",
      "======================================\n",
      "\n",
      "Once the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\n",
      "\n",
      "```\n",
      "from autogen import config_list_from_json  \n",
      "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \n",
      "from autogen import UserProxyAgent  \n",
      "  \n",
      "# config_list = config_list_from_json(\"OAI_CONFIG_LIST\")  \n",
      "  \n",
      "config_list = [  \n",
      "    {  \n",
      "        'model': 'gpt-4',  \n",
      "        'api_key': '<put your openai key here>',  \n",
      "    },  \n",
      "]\n",
      "```\n",
      "Step 3: Creating the GPTAssistantAgent\n",
      "======================================\n",
      "\n",
      "The next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None  \n",
      "    })\n",
      "```\n",
      "Step 4: Setting Up the UserProxyAgent\n",
      "=====================================\n",
      "\n",
      "The `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\n",
      "\n",
      "```\n",
      "user_proxy = UserProxyAgent(  \n",
      "    name=\"user_proxy\",  \n",
      "    code_execution_config={  \n",
      "        \"work_dir\": \"coding\"  \n",
      "    },  \n",
      "    human_input_mode=\"NEVER\")\n",
      "```\n",
      "Step 5: Initiating the Task\n",
      "===========================\n",
      "\n",
      "With both agents configured, you can now initiate a task. For example, to print “Hello World”, you would do the following:\n",
      "\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")\n",
      "```\n",
      "Step 6: Enabling Advanced Features\n",
      "==================================\n",
      "\n",
      "For more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None,  \n",
      "        \"tools\": [{\"type\": \"code_interpreter\"}],  \n",
      "    })\n",
      "```\n",
      "then test it with this:\n",
      "<a name=\"readme-top\"></a>\n",
      "\n",
      "[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\n",
      "[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\n",
      "![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\n",
      "[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\n",
      "[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\n",
      "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n",
      "\n",
      "\n",
      "# AutoGen\n",
      "[📚 Cite paper](#related-papers).\n",
      "<!-- <p align=\"center\">\n",
      "    <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>\n",
      "    <br>\n",
      "</p> -->\n",
      ":fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n",
      "\n",
      ":fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n",
      "\n",
      ":fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n",
      "\n",
      ":tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD).\n",
      "\n",
      ":tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n",
      "\n",
      "<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n",
      "\n",
      "<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n",
      "\n",
      ":tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff.\n",
      "\n",
      ":tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U).\n",
      "\n",
      ":tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023.\n",
      "\n",
      ":tada: Oct 03, 2023: AutoGen spins off from FLAML on GitHub and has a major paper update (first version on Aug 16).\n",
      "\n",
      "<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n",
      "\n",
      ":tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n",
      "\n",
      "<!--\n",
      ":fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n",
      "\n",
      ":fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n",
      "\n",
      ":fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## What is AutoGen\n",
      "\n",
      "AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\n",
      "\n",
      "![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n",
      "\n",
      "- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n",
      "- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n",
      "  the number of agents, and agent conversation topology.\n",
      "- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n",
      "- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n",
      "\n",
      "AutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Roadmaps\n",
      "\n",
      "To see what we are working on and what we plan to work on, please check our\n",
      "[Roadmap Issues](https://aka.ms/autogen-roadmap).\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Quickstart\n",
      "The easiest way to start playing is\n",
      "1. Click below to use the GitHub Codespace\n",
      "\n",
      "    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n",
      "\n",
      " 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n",
      " 3. Start playing with the notebooks!\n",
      "\n",
      "*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Get the price of gold and print a visual of the last 4 days closing price\")\n",
      "```\n",
      "![]()Conclusion\n",
      "==========\n",
      "\n",
      "The integration of AutoGen and OpenAI’s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\n",
      "\n",
      "Future Perspectives\n",
      "===================\n",
      "\n",
      "While there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\n",
      "\n",
      "[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\n",
      "\n",
      "--\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\n",
      "--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.5K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\n",
      "\n",
      "[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n",
      "### Option 1. Install and Run AutoGen in Docker\n",
      "\n",
      "Find detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n",
      "\n",
      "### Option 2. Install AutoGen Locally\n",
      "\n",
      "AutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n",
      "\n",
      "```bash\n",
      "pip install pyautogen\n",
      "```\n",
      "\n",
      "Minimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n",
      "\n",
      "<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n",
      "```bash\n",
      "pip install \"pyautogen[blendsearch]\"\n",
      "``` -->\n",
      "\n",
      "Find more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n",
      "\n",
      "<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n",
      "\n",
      "Even if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n",
      "\n",
      "For LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Multi-Agent Conversation Framework\n",
      "\n",
      "Autogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\n",
      "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n",
      "\n",
      "Features of this use case include:\n",
      "\n",
      "- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n",
      "- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n",
      "- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n",
      "\n",
      "For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n",
      "\n",
      "```python\n",
      "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
      "# Load LLM inference endpoints from an env variable or a file\n",
      "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n",
      "# and OAI_CONFIG_LIST_sample\n",
      "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
      "# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\n",
      "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
      "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
      "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n",
      "# This initiates an automated chat between the two agents to solve the task\n",
      "```\n",
      "\n",
      "This example can be run with\n",
      "\n",
      "```python\n",
      "python test/twoagent.py\n",
      "```\n",
      "\n",
      "After the repo is cloned.\n",
      "The figure below shows an example conversation flow with AutoGen.\n",
      "![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n",
      "\n",
      "Alternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\n",
      "Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Enhanced LLM Inferences\n",
      "\n",
      "Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n",
      "\n",
      "<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n",
      "\n",
      "```python\n",
      "# perform tuning for openai<1\n",
      "config, analysis = autogen.Completion.tune(\n",
      "    data=tune_data,\n",
      "    metric=\"success\",\n",
      "    mode=\"max\",\n",
      "    eval_func=eval_func,\n",
      "    inference_budget=0.05,\n",
      "    optimization_budget=3,\n",
      "    num_samples=-1,\n",
      ")\n",
      "# perform inference for a test instance\n",
      "response = autogen.Completion.create(context=test_instance, **config)\n",
      "```\n",
      "\n",
      "Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Documentation\n",
      "\n",
      "You can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n",
      "\n",
      "In addition, you can find:\n",
      "\n",
      "- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n",
      "\n",
      "- [Discord](https://aka.ms/autogen-dc)\n",
      "\n",
      "- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n",
      "\n",
      "- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Related Papers\n",
      "\n",
      "[AutoGen](https://arxiv.org/abs/2308.08155)\n",
      "\n",
      "```\n",
      "@inproceedings{wu2023autogen,\n",
      "      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n",
      "      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n",
      "      year={2023},\n",
      "      eprint={2308.08155},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.AI}\n",
      "}\n",
      "```\n",
      "\n",
      "[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n",
      "\n",
      "```\n",
      "@inproceedings{wang2023EcoOptiGen,\n",
      "    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n",
      "    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n",
      "    year={2023},\n",
      "    booktitle={AutoML'23},\n",
      "}\n",
      "```\n",
      "\n",
      "[MathChat](https://arxiv.org/abs/2306.01337)\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "AutoGen is a framework for developing LLM applications using multiple agents that converse with each other to solve tasks, enabling advanced, efficient, and customizable coding and development processes.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# reset the assistant. Always reset the assistant before starting a new conversation.\n",
    "assistant.reset()\n",
    "\n",
    "# given a problem, we use the ragproxyagent to generate a prompt to be sent to the assistant as the initial message.\n",
    "# the assistant receives the message and generates a response. The response will be sent back to the ragproxyagent for processing.\n",
    "# The conversation continues until the termination condition is met, in RetrieveChat, the termination condition when no human-in-loop is no code block detected.\n",
    "# With human-in-loop, the conversation will continue until the user says \"exit\".\n",
    "question = \"What is autogen?\" # \"what are the limitations?\" #  \"whom is autogen particularly useful for?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, \n",
    "    message=ragproxyagent.message_generator, \n",
    "    problem=question,\n",
    "    search_string=\"autogen\" # used as an extra filter for the embeddings search: in this case, we only want to search documents that contain \"autogen\"\n",
    ")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8be0f6c-90a6-49e2-ad8b-9e49ad6cb3a4",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent):\n",
    "\n",
    "AutoGen is a framework for developing LLM applications using multi-agent conversations, enabling complex task solving with minimal effort."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a79c69-42f0-4e2e-943f-c59b9b5ae0a9",
   "metadata": {},
   "source": [
    "## Ask the same question using normal UserProxyAgent / AssistantAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f78cb378-c080-4b7b-a377-b1d8138ed948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is Autogen?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to user_proxy):\n",
      "\n",
      "\"Autogen\" can refer to different concepts depending on the context in which it's used. Here are a few possibilities:\n",
      "\n",
      "1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or scripts that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, or even documentation from a set of specifications or templates. This automation can significantly speed up development processes and ensure consistency across large projects.\n",
      "\n",
      "2. **Medical or Biological Context**: The term could also be used in a medical or biological context, referring to autogenous or autogenic training or therapy. This is a technique that teaches your body to respond to your verbal commands, helping to control bodily functions that are normally considered automatic, such as heart rate or blood pressure. It's a form of self-hypnosis or relaxation technique.\n",
      "\n",
      "3. **Specific Software or Libraries**: There are specific software packages or libraries named \"Autogen\" designed for various purposes, such as generating code. An example is the GNU AutoGen, a tool designed to simplify the creation and maintenance of programs that contain large amounts of repetitive text. It is especially useful for creating the code and documentation that can be automatically generated from a set of templates.\n",
      "\n",
      "4. **Other Industries**: In other contexts, \"Autogen\" might refer to companies, products, or technologies that emphasize \"auto-generation\" or automation in their processes or functionalities.\n",
      "\n",
      "Without more specific context, it's challenging to provide a precise definition. If you have a particular area or usage in mind, please provide more details for a more targeted explanation.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33muser_proxy\u001b[0m (to assistant):\n",
      "\n",
      "What is Autogen?\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mautogen\u001b[39;00m\n\u001b[1;32m      3\u001b[0m user_proxy \u001b[38;5;241m=\u001b[39m autogen\u001b[38;5;241m.\u001b[39mUserProxyAgent(\n\u001b[1;32m      4\u001b[0m     name                       \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_proxy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     human_input_mode           \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEVER\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;66;03m# NEVER / ALWAYS / TERMINATE\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     llm_config\u001b[38;5;241m=\u001b[39mllm_config\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m chat_res \u001b[38;5;241m=\u001b[39m \u001b[43muser_proxy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitiate_chat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43massistant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhat is Autogen?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43msummary_method\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mreflection_with_llm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:990\u001b[0m, in \u001b[0;36mConversableAgent.initiate_chat\u001b[0;34m(self, recipient, clear_history, silent, cache, max_turns, summary_method, summary_args, message, **kwargs)\u001b[0m\n\u001b[1;32m    988\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    989\u001b[0m         msg2send \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_init_message(message, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg2send\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecipient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    991\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_summarize_chat(\n\u001b[1;32m    992\u001b[0m     summary_method,\n\u001b[1;32m    993\u001b[0m     summary_args,\n\u001b[1;32m    994\u001b[0m     recipient,\n\u001b[1;32m    995\u001b[0m     cache\u001b[38;5;241m=\u001b[39mcache,\n\u001b[1;32m    996\u001b[0m )\n\u001b[1;32m    997\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;28mself\u001b[39m, recipient]:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:631\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    629\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 631\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:793\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    791\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:631\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    629\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 631\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:793\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    791\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_reply(messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_messages[sender], sender\u001b[38;5;241m=\u001b[39msender)\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 793\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:631\u001b[0m, in \u001b[0;36mConversableAgent.send\u001b[0;34m(self, message, recipient, request_reply, silent)\u001b[0m\n\u001b[1;32m    629\u001b[0m valid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_oai_message(message, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, recipient)\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m valid:\n\u001b[0;32m--> 631\u001b[0m     \u001b[43mrecipient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreceive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrequest_reply\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMessage can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt be converted into a valid ChatCompletion message. Either content or function_call must be provided.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    635\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:791\u001b[0m, in \u001b[0;36mConversableAgent.receive\u001b[0;34m(self, message, sender, request_reply, silent)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m request_reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreply_at_receive[sender] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 791\u001b[0m reply \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_reply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_messages\u001b[49m\u001b[43m[\u001b[49m\u001b[43msender\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reply \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    793\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msend(reply, sender, silent\u001b[38;5;241m=\u001b[39msilent)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1912\u001b[0m, in \u001b[0;36mConversableAgent.generate_reply\u001b[0;34m(self, messages, sender, **kwargs)\u001b[0m\n\u001b[1;32m   1910\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_match_trigger(reply_func_tuple[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrigger\u001b[39m\u001b[38;5;124m\"\u001b[39m], sender):\n\u001b[0;32m-> 1912\u001b[0m     final, reply \u001b[38;5;241m=\u001b[39m \u001b[43mreply_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msender\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreply_func_tuple\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfig\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1913\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m final:\n\u001b[1;32m   1914\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reply\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1278\u001b[0m, in \u001b[0;36mConversableAgent.generate_oai_reply\u001b[0;34m(self, messages, sender, config)\u001b[0m\n\u001b[1;32m   1276\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m messages \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1277\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_oai_messages[sender]\n\u001b[0;32m-> 1278\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_oai_reply_from_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_oai_system_message\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient_cache\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, extracted_response)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/agentchat/conversable_agent.py:1297\u001b[0m, in \u001b[0;36mConversableAgent._generate_oai_reply_from_client\u001b[0;34m(self, llm_client, messages, cache)\u001b[0m\n\u001b[1;32m   1294\u001b[0m         all_messages\u001b[38;5;241m.\u001b[39mappend(message)\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# TODO: #1143 handle token limit exceeded error\u001b[39;00m\n\u001b[0;32m-> 1297\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1301\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1302\u001b[0m extracted_response \u001b[38;5;241m=\u001b[39m llm_client\u001b[38;5;241m.\u001b[39mextract_text_or_completion_object(response)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extracted_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/oai/client.py:627\u001b[0m, in \u001b[0;36mOpenAIWrapper.create\u001b[0;34m(self, **config)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     request_ts \u001b[38;5;241m=\u001b[39m get_current_ts()\n\u001b[0;32m--> 627\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m APITimeoutError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    629\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/autogen/oai/client.py:278\u001b[0m, in \u001b[0;36mOpenAIClient.create\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    276\u001b[0m     params \u001b[38;5;241m=\u001b[39m params\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    277\u001b[0m     params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/resources/chat/completions.py:667\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    615\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    665\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    666\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    671\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    672\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    673\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    674\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    675\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    676\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    677\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    678\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    679\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    680\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py:1213\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1200\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1201\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1210\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1211\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1212\u001b[0m     )\n\u001b[0;32m-> 1213\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py:902\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    894\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    895\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    900\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    901\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 902\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    903\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    904\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    905\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    906\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    907\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/openai/_base_client.py:931\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    928\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcustom_auth\n\u001b[1;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 931\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    933\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    934\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    935\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    936\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    937\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:914\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    906\u001b[0m follow_redirects \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    907\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfollow_redirects\n\u001b[1;32m    908\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(follow_redirects, UseClientDefault)\n\u001b[1;32m    909\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m follow_redirects\n\u001b[1;32m    910\u001b[0m )\n\u001b[1;32m    912\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 914\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:942\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    939\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    941\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 942\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    947\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:979\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    977\u001b[0m     hook(request)\n\u001b[0;32m--> 979\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    981\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_client.py:1015\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1010\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1011\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1012\u001b[0m     )\n\u001b[1;32m   1014\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1015\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1019\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpx/_transports/default.py:233\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    220\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    221\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    222\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    231\u001b[0m )\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 233\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    238\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    239\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    240\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    241\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    242\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1295\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1292\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1293\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1294\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1295\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m~/miniconda3/envs/autogen/lib/python3.11/ssl.py:1168\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1167\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1168\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1169\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1170\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import autogen\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 1,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Autogen?\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab0a9a-e7c3-49a6-ab45-cc55cc980d20",
   "metadata": {},
   "source": [
    "### assistant (to user_proxy):\n",
    "\n",
    "\"Autogen\" can refer to different concepts depending on the context in which it's used. Here are a few possibilities:\n",
    "\n",
    "1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or scripts that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, client libraries, or even documentation from a predefined template or source code annotations. This automation can significantly speed up development processes and ensure consistency across large projects.\n",
    "\n",
    "2. **Medical or Biological Term**: The term \"autogenic\" or \"autogenesis\" can refer to processes or responses that originate from within an organism, cell, or system. For example, autogenic training is a relaxation technique developed by German psychiatrist Johannes Heinrich Schultz, which involves self-induced suggestive relaxation and stress reduction.\n",
    "\n",
    "3. **Specific Software or Libraries**: There are specific tools or libraries named \"Autogen\" designed for various purposes, such as generating code from other specifications. For example, in the context of GNU/Linux systems, there is a tool called \"AutoGen\" that is used to simplify the creation and maintenance of programs that contain large amounts of repetitive text.\n",
    "\n",
    "Without more context, it's challenging to provide a precise definition of \"Autogen.\" If you have a specific context in mind, please provide more details for a more accurate explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e7d9fc-5b34-4375-a8b3-7f79d4832c69",
   "metadata": {},
   "source": [
    "## Customize Embedding Function with Azure text-embedding-3-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddd0b833-df4b-4353-acda-a53b8cf45440",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "azure_openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    model_name=os.environ[\"EMBEDDING_ADA_003_LARGE\"],\n",
    "    api_base=os.environ[\"AZURE_OPENAI_ENDPOINT_CA\"],\n",
    "    api_key=os.environ[\"AZURE_OPENAI_API_KEY_CA\"],\n",
    "    api_type=os.environ[\"OPENAI_API_TYPE\"],\n",
    "    api_version=os.environ[\"OPENAI_API_VERSION\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e404270-87be-415d-a946-f3d210d7a236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<autogen.agentchat.contrib.retrieve_user_proxy_agent.RetrieveUserProxyAgent at 0x7fd2f2b7f690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import chromadb\n",
    "ragproxyagent = RetrieveUserProxyAgent(\n",
    "    name=\"ragproxyagent\",\n",
    "    human_input_mode=\"NEVER\",\n",
    "    max_consecutive_auto_reply=3,\n",
    "    retrieve_config={\n",
    "        \"task\": \"qa\",\n",
    "        \"docs_path\": [\n",
    "            \"https://raw.githubusercontent.com/microsoft/autogen/main/README.md\",\n",
    "            \"https://drlee.io/harnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c\",\n",
    "            \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Examples/Integrate%20-%20Spark.md\",\n",
    "            # \"https://raw.githubusercontent.com/microsoft/FLAML/main/website/docs/Research.md\",\n",
    "            # os.path.join(os.path.abspath(\"\"), \"..\", \"website\", \"docs\"),\n",
    "        ],\n",
    "        \"custom_text_types\": TEXT_FORMATS, # [\"mdx\"],\n",
    "        \"chunk_token_size\": 2000,\n",
    "        \"model\": \"gpt-4\", # https://cookbook.openai.com/examples/how_to_count_tokens_with_tiktoken\n",
    "        \"client\": chromadb.PersistentClient(path=\"/tmp/chromadb\"),\n",
    "        \"collection_name\": \"mauromi_collection_043\",\n",
    "        \"embedding_function\": azure_openai_ef,\n",
    "        \"get_or_create\": False,  # set to False if you don't want to reuse an existing collection, but you'll need to remove the collection manually\n",
    "    },\n",
    "    code_execution_config=False,  # set to False if you don't want to execute the code\n",
    ")\n",
    "\n",
    "ragproxyagent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fba57535-119e-4d72-b072-a73ad72f4fd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to create collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_1', 'doc_3', 'doc_4', 'doc_0', 'doc_2']]\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_3 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_4 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: what are the enhanced LLM inference features?\n",
      "\n",
      "Context is: \n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n",
      "### Option 1. Install and Run AutoGen in Docker\n",
      "\n",
      "Find detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n",
      "\n",
      "### Option 2. Install AutoGen Locally\n",
      "\n",
      "AutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n",
      "\n",
      "```bash\n",
      "pip install pyautogen\n",
      "```\n",
      "\n",
      "Minimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n",
      "\n",
      "<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n",
      "```bash\n",
      "pip install \"pyautogen[blendsearch]\"\n",
      "``` -->\n",
      "\n",
      "Find more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n",
      "\n",
      "<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n",
      "\n",
      "Even if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n",
      "\n",
      "For LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Multi-Agent Conversation Framework\n",
      "\n",
      "Autogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\n",
      "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n",
      "\n",
      "Features of this use case include:\n",
      "\n",
      "- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n",
      "- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n",
      "- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n",
      "\n",
      "For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n",
      "\n",
      "```python\n",
      "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
      "# Load LLM inference endpoints from an env variable or a file\n",
      "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n",
      "# and OAI_CONFIG_LIST_sample\n",
      "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
      "# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\n",
      "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
      "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
      "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n",
      "# This initiates an automated chat between the two agents to solve the task\n",
      "```\n",
      "\n",
      "This example can be run with\n",
      "\n",
      "```python\n",
      "python test/twoagent.py\n",
      "```\n",
      "\n",
      "After the repo is cloned.\n",
      "The figure below shows an example conversation flow with AutoGen.\n",
      "![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n",
      "\n",
      "Alternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\n",
      "Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Enhanced LLM Inferences\n",
      "\n",
      "Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n",
      "\n",
      "<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n",
      "\n",
      "```python\n",
      "# perform tuning for openai<1\n",
      "config, analysis = autogen.Completion.tune(\n",
      "    data=tune_data,\n",
      "    metric=\"success\",\n",
      "    mode=\"max\",\n",
      "    eval_func=eval_func,\n",
      "    inference_budget=0.05,\n",
      "    optimization_budget=3,\n",
      "    num_samples=-1,\n",
      ")\n",
      "# perform inference for a test instance\n",
      "response = autogen.Completion.create(context=test_instance, **config)\n",
      "```\n",
      "\n",
      "Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Documentation\n",
      "\n",
      "You can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n",
      "\n",
      "In addition, you can find:\n",
      "\n",
      "- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n",
      "\n",
      "- [Discord](https://aka.ms/autogen-dc)\n",
      "\n",
      "- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n",
      "\n",
      "- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Related Papers\n",
      "\n",
      "[AutoGen](https://arxiv.org/abs/2308.08155)\n",
      "\n",
      "```\n",
      "@inproceedings{wu2023autogen,\n",
      "      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n",
      "      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n",
      "      year={2023},\n",
      "      eprint={2308.08155},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.AI}\n",
      "}\n",
      "```\n",
      "\n",
      "[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n",
      "\n",
      "```\n",
      "@inproceedings{wang2023EcoOptiGen,\n",
      "    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n",
      "    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n",
      "    year={2023},\n",
      "    booktitle={AutoML'23},\n",
      "}\n",
      "```\n",
      "\n",
      "[MathChat](https://arxiv.org/abs/2306.01337)\n",
      "# Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\n",
      "\n",
      "Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\n",
      "================================================================================================\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "·[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\n",
      "\n",
      "3 min read·Dec 19, 2023--\n",
      "\n",
      "Listen\n",
      "\n",
      "Share\n",
      "\n",
      "![]()Introduction\n",
      "============\n",
      "\n",
      "In the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI’s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\n",
      "\n",
      "Step 1: Setting Up Your Environment\n",
      "===================================\n",
      "\n",
      "Before diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\n",
      "\n",
      "```\n",
      "!pip install pyautogen==0.2.0b5\n",
      "```\n",
      "Step 2: Importing the Required Modules\n",
      "======================================\n",
      "\n",
      "Once the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\n",
      "\n",
      "```\n",
      "from autogen import config_list_from_json  \n",
      "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \n",
      "from autogen import UserProxyAgent  \n",
      "  \n",
      "# config_list = config_list_from_json(\"OAI_CONFIG_LIST\")  \n",
      "  \n",
      "config_list = [  \n",
      "    {  \n",
      "        'model': 'gpt-4',  \n",
      "        'api_key': '<put your openai key here>',  \n",
      "    },  \n",
      "]\n",
      "```\n",
      "Step 3: Creating the GPTAssistantAgent\n",
      "======================================\n",
      "\n",
      "The next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None  \n",
      "    })\n",
      "```\n",
      "Step 4: Setting Up the UserProxyAgent\n",
      "=====================================\n",
      "\n",
      "The `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\n",
      "\n",
      "```\n",
      "user_proxy = UserProxyAgent(  \n",
      "    name=\"user_proxy\",  \n",
      "    code_execution_config={  \n",
      "        \"work_dir\": \"coding\"  \n",
      "    },  \n",
      "    human_input_mode=\"NEVER\")\n",
      "```\n",
      "Step 5: Initiating the Task\n",
      "===========================\n",
      "\n",
      "With both agents configured, you can now initiate a task. For example, to print “Hello World”, you would do the following:\n",
      "\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")\n",
      "```\n",
      "Step 6: Enabling Advanced Features\n",
      "==================================\n",
      "\n",
      "For more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None,  \n",
      "        \"tools\": [{\"type\": \"code_interpreter\"}],  \n",
      "    })\n",
      "```\n",
      "then test it with this:\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Get the price of gold and print a visual of the last 4 days closing price\")\n",
      "```\n",
      "![]()Conclusion\n",
      "==========\n",
      "\n",
      "The integration of AutoGen and OpenAI’s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\n",
      "\n",
      "Future Perspectives\n",
      "===================\n",
      "\n",
      "While there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\n",
      "\n",
      "[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\n",
      "\n",
      "--\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\n",
      "--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.5K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\n",
      "\n",
      "[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\n",
      "<a name=\"readme-top\"></a>\n",
      "\n",
      "[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\n",
      "[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\n",
      "![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\n",
      "[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\n",
      "[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\n",
      "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n",
      "\n",
      "\n",
      "# AutoGen\n",
      "[📚 Cite paper](#related-papers).\n",
      "<!-- <p align=\"center\">\n",
      "    <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>\n",
      "    <br>\n",
      "</p> -->\n",
      ":fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n",
      "\n",
      ":fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n",
      "\n",
      ":fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n",
      "\n",
      ":tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD).\n",
      "\n",
      ":tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n",
      "\n",
      "<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n",
      "\n",
      "<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n",
      "\n",
      ":tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff.\n",
      "\n",
      ":tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U).\n",
      "\n",
      ":tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023.\n",
      "\n",
      ":tada: Oct 03, 2023: AutoGen spins off from FLAML on GitHub and has a major paper update (first version on Aug 16).\n",
      "\n",
      "<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n",
      "\n",
      ":tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n",
      "\n",
      "<!--\n",
      ":fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n",
      "\n",
      ":fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n",
      "\n",
      ":fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## What is AutoGen\n",
      "\n",
      "AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\n",
      "\n",
      "![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n",
      "\n",
      "- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n",
      "- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n",
      "  the number of agents, and agent conversation topology.\n",
      "- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n",
      "- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n",
      "\n",
      "AutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Roadmaps\n",
      "\n",
      "To see what we are working on and what we plan to work on, please check our\n",
      "[Roadmap Issues](https://aka.ms/autogen-roadmap).\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Quickstart\n",
      "The easiest way to start playing is\n",
      "1. Click below to use the GitHub Codespace\n",
      "\n",
      "    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n",
      "\n",
      " 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n",
      " 3. Start playing with the notebooks!\n",
      "\n",
      "*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "Caching, error handling, multi-config inference, templating.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"what are the enhanced LLM inference features?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, message=ragproxyagent.message_generator,\n",
    "    problem=question,\n",
    "    search_string=\"autogen\" # used as an extra filter for the embeddings search: in this case, we only want to search documents that contain \"autogen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bfc340-1d5e-4a5a-82cf-c8b93fa6c444",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent)\n",
    "\n",
    "Caching, error handling, multi-config inference, templating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88463cd7-7194-48ce-88c4-e16c1f0863fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 20 is greater than number of elements in index 6, updating n_results = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_ids:  [['doc_0', 'doc_1', 'doc_3', 'doc_4', 'doc_2']]\n",
      "\u001b[32mAdding doc_id doc_0 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_1 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_3 to context.\u001b[0m\n",
      "\u001b[32mAdding doc_id doc_4 to context.\u001b[0m\n",
      "\u001b[33mragproxyagent\u001b[0m (to assistant):\n",
      "\n",
      "You're a retrieve augmented chatbot. You answer user's questions based on your own knowledge and the\n",
      "context provided by the user.\n",
      "If you can't answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\n",
      "You must give as short an answer as possible.\n",
      "\n",
      "User's question is: What Python versions are supported by Autogen?\n",
      "\n",
      "Context is: <a name=\"readme-top\"></a>\n",
      "\n",
      "[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\n",
      "[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\n",
      "![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\n",
      "[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\n",
      "[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\n",
      "[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\n",
      "\n",
      "\n",
      "# AutoGen\n",
      "[📚 Cite paper](#related-papers).\n",
      "<!-- <p align=\"center\">\n",
      "    <img src=\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\"  width=200>\n",
      "    <br>\n",
      "</p> -->\n",
      ":fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What's next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital's AI Ascent (Mar 26).\n",
      "\n",
      ":fire: Mar 3, 2024: What's new in AutoGen? 📰[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); 📺[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\n",
      "\n",
      ":fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\n",
      "\n",
      ":tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD).\n",
      "\n",
      ":tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\n",
      "\n",
      "<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\n",
      "\n",
      "<!-- :fire: Nov 11: OpenAI's Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\n",
      "\n",
      ":tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff.\n",
      "\n",
      ":tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U).\n",
      "\n",
      ":tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023.\n",
      "\n",
      ":tada: Oct 03, 2023: AutoGen spins off from FLAML on GitHub and has a major paper update (first version on Aug 16).\n",
      "\n",
      "<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\n",
      "\n",
      ":tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\n",
      "\n",
      "<!--\n",
      ":fire: FLAML is highlighted in OpenAI's [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\n",
      "\n",
      ":fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\n",
      "\n",
      ":fire: FLAML supports Code-First AutoML & Tuning – Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## What is AutoGen\n",
      "\n",
      "AutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\n",
      "\n",
      "![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\n",
      "\n",
      "- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\n",
      "- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\n",
      "  the number of agents, and agent conversation topology.\n",
      "- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\n",
      "- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\n",
      "\n",
      "AutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Roadmaps\n",
      "\n",
      "To see what we are working on and what we plan to work on, please check our\n",
      "[Roadmap Issues](https://aka.ms/autogen-roadmap).\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Quickstart\n",
      "The easiest way to start playing is\n",
      "1. Click below to use the GitHub Codespace\n",
      "\n",
      "    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\n",
      "\n",
      " 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\n",
      " 3. Start playing with the notebooks!\n",
      "\n",
      "*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## [Installation](https://microsoft.github.io/autogen/docs/Installation)\n",
      "### Option 1. Install and Run AutoGen in Docker\n",
      "\n",
      "Find detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\n",
      "\n",
      "### Option 2. Install AutoGen Locally\n",
      "\n",
      "AutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\n",
      "\n",
      "```bash\n",
      "pip install pyautogen\n",
      "```\n",
      "\n",
      "Minimal dependencies are installed without extra options. You can install extra options based on the feature you need.\n",
      "\n",
      "<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\n",
      "```bash\n",
      "pip install \"pyautogen[blendsearch]\"\n",
      "``` -->\n",
      "\n",
      "Find more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\n",
      "\n",
      "<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\n",
      "\n",
      "Even if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\n",
      "\n",
      "For LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Multi-Agent Conversation Framework\n",
      "\n",
      "Autogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\n",
      "By automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\n",
      "\n",
      "Features of this use case include:\n",
      "\n",
      "- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\n",
      "- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\n",
      "- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\n",
      "\n",
      "For [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\n",
      "\n",
      "```python\n",
      "from autogen import AssistantAgent, UserProxyAgent, config_list_from_json\n",
      "# Load LLM inference endpoints from an env variable or a file\n",
      "# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\n",
      "# and OAI_CONFIG_LIST_sample\n",
      "config_list = config_list_from_json(env_or_file=\"OAI_CONFIG_LIST\")\n",
      "# You can also set config_list directly as a list, for example, config_list = [{'model': 'gpt-4', 'api_key': '<your OpenAI API key here>'},]\n",
      "assistant = AssistantAgent(\"assistant\", llm_config={\"config_list\": config_list})\n",
      "user_proxy = UserProxyAgent(\"user_proxy\", code_execution_config={\"work_dir\": \"coding\", \"use_docker\": False}) # IMPORTANT: set to True to run code in docker, recommended\n",
      "user_proxy.initiate_chat(assistant, message=\"Plot a chart of NVDA and TESLA stock price change YTD.\")\n",
      "# This initiates an automated chat between the two agents to solve the task\n",
      "```\n",
      "\n",
      "This example can be run with\n",
      "\n",
      "```python\n",
      "python test/twoagent.py\n",
      "```\n",
      "\n",
      "After the repo is cloned.\n",
      "The figure below shows an example conversation flow with AutoGen.\n",
      "![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\n",
      "\n",
      "Alternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\n",
      "Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Enhanced LLM Inferences\n",
      "\n",
      "Autogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\n",
      "\n",
      "<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\n",
      "\n",
      "```python\n",
      "# perform tuning for openai<1\n",
      "config, analysis = autogen.Completion.tune(\n",
      "    data=tune_data,\n",
      "    metric=\"success\",\n",
      "    mode=\"max\",\n",
      "    eval_func=eval_func,\n",
      "    inference_budget=0.05,\n",
      "    optimization_budget=3,\n",
      "    num_samples=-1,\n",
      ")\n",
      "# perform inference for a test instance\n",
      "response = autogen.Completion.create(context=test_instance, **config)\n",
      "```\n",
      "\n",
      "Please find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Documentation\n",
      "\n",
      "You can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\n",
      "\n",
      "In addition, you can find:\n",
      "\n",
      "- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\n",
      "\n",
      "- [Discord](https://aka.ms/autogen-dc)\n",
      "\n",
      "- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\n",
      "\n",
      "- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\n",
      "\n",
      "<p align=\"right\" style=\"font-size: 14px; color: #555; margin-top: 20px;\">\n",
      "  <a href=\"#readme-top\" style=\"text-decoration: none; color: blue; font-weight: bold;\">\n",
      "    ↑ Back to Top ↑\n",
      "  </a>\n",
      "</p>\n",
      "\n",
      "## Related Papers\n",
      "\n",
      "[AutoGen](https://arxiv.org/abs/2308.08155)\n",
      "\n",
      "```\n",
      "@inproceedings{wu2023autogen,\n",
      "      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\n",
      "      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\n",
      "      year={2023},\n",
      "      eprint={2308.08155},\n",
      "      archivePrefix={arXiv},\n",
      "      primaryClass={cs.AI}\n",
      "}\n",
      "```\n",
      "\n",
      "[EcoOptiGen](https://arxiv.org/abs/2303.04673)\n",
      "\n",
      "```\n",
      "@inproceedings{wang2023EcoOptiGen,\n",
      "    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\n",
      "    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\n",
      "    year={2023},\n",
      "    booktitle={AutoML'23},\n",
      "}\n",
      "```\n",
      "\n",
      "[MathChat](https://arxiv.org/abs/2306.01337)\n",
      "# Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\n",
      "\n",
      "Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\n",
      "\n",
      "![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\n",
      "================================================================================================\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "·[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\n",
      "\n",
      "3 min read·Dec 19, 2023--\n",
      "\n",
      "Listen\n",
      "\n",
      "Share\n",
      "\n",
      "![]()Introduction\n",
      "============\n",
      "\n",
      "In the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI’s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\n",
      "\n",
      "Step 1: Setting Up Your Environment\n",
      "===================================\n",
      "\n",
      "Before diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\n",
      "\n",
      "```\n",
      "!pip install pyautogen==0.2.0b5\n",
      "```\n",
      "Step 2: Importing the Required Modules\n",
      "======================================\n",
      "\n",
      "Once the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\n",
      "\n",
      "```\n",
      "from autogen import config_list_from_json  \n",
      "from autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \n",
      "from autogen import UserProxyAgent  \n",
      "  \n",
      "# config_list = config_list_from_json(\"OAI_CONFIG_LIST\")  \n",
      "  \n",
      "config_list = [  \n",
      "    {  \n",
      "        'model': 'gpt-4',  \n",
      "        'api_key': '<put your openai key here>',  \n",
      "    },  \n",
      "]\n",
      "```\n",
      "Step 3: Creating the GPTAssistantAgent\n",
      "======================================\n",
      "\n",
      "The next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None  \n",
      "    })\n",
      "```\n",
      "Step 4: Setting Up the UserProxyAgent\n",
      "=====================================\n",
      "\n",
      "The `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\n",
      "\n",
      "```\n",
      "user_proxy = UserProxyAgent(  \n",
      "    name=\"user_proxy\",  \n",
      "    code_execution_config={  \n",
      "        \"work_dir\": \"coding\"  \n",
      "    },  \n",
      "    human_input_mode=\"NEVER\")\n",
      "```\n",
      "Step 5: Initiating the Task\n",
      "===========================\n",
      "\n",
      "With both agents configured, you can now initiate a task. For example, to print “Hello World”, you would do the following:\n",
      "\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Print hello world\")\n",
      "```\n",
      "Step 6: Enabling Advanced Features\n",
      "==================================\n",
      "\n",
      "For more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\n",
      "\n",
      "```\n",
      "gpt_assistant = GPTAssistantAgent(  \n",
      "    name=\"assistant\",  \n",
      "    llm_config={  \n",
      "        \"config_list\": config_list,  \n",
      "        \"assistant_id\": None,  \n",
      "        \"tools\": [{\"type\": \"code_interpreter\"}],  \n",
      "    })\n",
      "```\n",
      "then test it with this:\n",
      "```\n",
      "user_proxy.initiate_chat(gpt_assistant, message=\"Get the price of gold and print a visual of the last 4 days closing price\")\n",
      "```\n",
      "![]()Conclusion\n",
      "==========\n",
      "\n",
      "The integration of AutoGen and OpenAI’s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\n",
      "\n",
      "Future Perspectives\n",
      "===================\n",
      "\n",
      "While there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\n",
      "\n",
      "[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\n",
      "\n",
      "--\n",
      "\n",
      "[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\n",
      "--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.5K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\n",
      "\n",
      "[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33massistant\u001b[0m (to ragproxyagent):\n",
      "\n",
      "3.8 | 3.9 | 3.10 | 3.11 | 3.12\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "question = \"What Python versions are supported by Autogen?\"\n",
    "answer = ragproxyagent.initiate_chat(\n",
    "    assistant, message=ragproxyagent.message_generator,\n",
    "    problem=question,\n",
    "    search_string=\"autogen\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47975832-5696-45bf-adf6-5b6dfb3b8c14",
   "metadata": {},
   "source": [
    "### assistant (to ragproxyagent):\n",
    "\n",
    "3.8 | 3.9 | 3.10 | 3.11 | 3.12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f9abda-4599-4f32-9a53-dd8cbf997604",
   "metadata": {},
   "source": [
    "# Check the cache DB with SQL Lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ae1a5ca8-4ab1-4288-8ac3-40ec35724aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in this DB: [('Settings',), ('Cache',)]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++ ROW 1 ++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "(1, '{\"messages\": [{\"content\": \"You are a helpful assistant.\", \"role\": \"system\"}, {\"content\": \"You\\'re a retrieve augmented chatbot. You answer user\\'s questions based on your own knowledge and the\\\\ncontext provided by the user.\\\\nIf you can\\'t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\\\nYou must give as short an answer as possible.\\\\n\\\\nUser\\'s question is: What is autogen?\\\\n\\\\nContext is: # Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\\\\n\\\\nHarnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\\\\n================================================================================================\\\\n\\\\n[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\\\\n\\\\n\\\\u00b7[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\\\\n\\\\n3 min read\\\\u00b7Dec 19, 2023--\\\\n\\\\nListen\\\\n\\\\nShare\\\\n\\\\n![]()Introduction\\\\n============\\\\n\\\\nIn the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI\\\\u2019s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\\\\n\\\\nStep 1: Setting Up Your Environment\\\\n===================================\\\\n\\\\nBefore diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\\\\n\\\\n```\\\\n!pip install pyautogen==0.2.0b5\\\\n```\\\\nStep 2: Importing the Required Modules\\\\n======================================\\\\n\\\\nOnce the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\\\\n\\\\n```\\\\nfrom autogen import config_list_from_json  \\\\nfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \\\\nfrom autogen import UserProxyAgent  \\\\n  \\\\n# config_list = config_list_from_json(\\\\\"OAI_CONFIG_LIST\\\\\")  \\\\n  \\\\nconfig_list = [  \\\\n    {  \\\\n        \\'model\\': \\'gpt-4\\',  \\\\n        \\'api_key\\': \\'<put your openai key here>\\',  \\\\n    },  \\\\n]\\\\n```\\\\nStep 3: Creating the GPTAssistantAgent\\\\n======================================\\\\n\\\\nThe next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\\\\n\\\\n```\\\\ngpt_assistant = GPTAssistantAgent(  \\\\n    name=\\\\\"assistant\\\\\",  \\\\n    llm_config={  \\\\n        \\\\\"config_list\\\\\": config_list,  \\\\n        \\\\\"assistant_id\\\\\": None  \\\\n    })\\\\n```\\\\nStep 4: Setting Up the UserProxyAgent\\\\n=====================================\\\\n\\\\nThe `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\\\\n\\\\n```\\\\nuser_proxy = UserProxyAgent(  \\\\n    name=\\\\\"user_proxy\\\\\",  \\\\n    code_execution_config={  \\\\n        \\\\\"work_dir\\\\\": \\\\\"coding\\\\\"  \\\\n    },  \\\\n    human_input_mode=\\\\\"NEVER\\\\\")\\\\n```\\\\nStep 5: Initiating the Task\\\\n===========================\\\\n\\\\nWith both agents configured, you can now initiate a task. For example, to print \\\\u201cHello World\\\\u201d, you would do the following:\\\\n\\\\n```\\\\nuser_proxy.initiate_chat(gpt_assistant, message=\\\\\"Print hello world\\\\\")\\\\n```\\\\nStep 6: Enabling Advanced Features\\\\n==================================\\\\n\\\\nFor more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\\\\n\\\\n```\\\\ngpt_assistant = GPTAssistantAgent(  \\\\n    name=\\\\\"assistant\\\\\",  \\\\n    llm_config={  \\\\n        \\\\\"config_list\\\\\": config_list,  \\\\n        \\\\\"assistant_id\\\\\": None,  \\\\n        \\\\\"tools\\\\\": [{\\\\\"type\\\\\": \\\\\"code_interpreter\\\\\"}],  \\\\n    })\\\\n```\\\\nthen test it with this:\\\\n<a name=\\\\\"readme-top\\\\\"></a>\\\\n\\\\n[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\\\\n[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\\\\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\\\\n[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\\\\n[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\\\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\\\\n\\\\n\\\\n# AutoGen\\\\n[\\\\ud83d\\\\udcda Cite paper](#related-papers).\\\\n<!-- <p align=\\\\\"center\\\\\">\\\\n    <img src=\\\\\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\\\\\"  width=200>\\\\n    <br>\\\\n</p> -->\\\\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What\\'s next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital\\'s AI Ascent (Mar 26).\\\\n\\\\n:fire: Mar 3, 2024: What\\'s new in AutoGen? \\\\ud83d\\\\udcf0[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); \\\\ud83d\\\\udcfa[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\\\\n\\\\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\\\\n\\\\n:tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD).\\\\n\\\\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\\\\n\\\\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\\\\n\\\\n<!-- :fire: Nov 11: OpenAI\\'s Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\\\\n\\\\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff.\\\\n\\\\n:tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U).\\\\n\\\\n:tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023.\\\\n\\\\n:tada: Oct 03, 2023: AutoGen spins off from FLAML on GitHub and has a major paper update (first version on Aug 16).\\\\n\\\\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\\\\n\\\\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\\\\n\\\\n<!--\\\\n:fire: FLAML is highlighted in OpenAI\\'s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\\\\n\\\\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\\\\n\\\\n:fire: FLAML supports Code-First AutoML & Tuning \\\\u2013 Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## What is AutoGen\\\\n\\\\nAutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\\\n\\\\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\\\\n\\\\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\\\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\\\n  the number of agents, and agent conversation topology.\\\\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\\\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\\\n\\\\nAutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Roadmaps\\\\n\\\\nTo see what we are working on and what we plan to work on, please check our\\\\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Quickstart\\\\nThe easiest way to start playing is\\\\n1. Click below to use the GitHub Codespace\\\\n\\\\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\\\\n\\\\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\\\\n 3. Start playing with the notebooks!\\\\n\\\\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\\\\n```\\\\nuser_proxy.initiate_chat(gpt_assistant, message=\\\\\"Get the price of gold and print a visual of the last 4 days closing price\\\\\")\\\\n```\\\\n![]()Conclusion\\\\n==========\\\\n\\\\nThe integration of AutoGen and OpenAI\\\\u2019s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\\\\n\\\\nFuture Perspectives\\\\n===================\\\\n\\\\nWhile there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\\\\n\\\\n[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\\\\n\\\\n--\\\\n\\\\n[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\\\\n--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.5K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\\\\n\\\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\\\\n### Option 1. Install and Run AutoGen in Docker\\\\n\\\\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\\\\n\\\\n### Option 2. Install AutoGen Locally\\\\n\\\\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\\\\n\\\\n```bash\\\\npip install pyautogen\\\\n```\\\\n\\\\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\\\\n\\\\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\\\\n```bash\\\\npip install \\\\\"pyautogen[blendsearch]\\\\\"\\\\n``` -->\\\\n\\\\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\\\\n\\\\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\\\\n\\\\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\\\\n\\\\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Multi-Agent Conversation Framework\\\\n\\\\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\\\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\\\\n\\\\nFeatures of this use case include:\\\\n\\\\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\\\\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\\\\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\\\\n\\\\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\\\n\\\\n```python\\\\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\\\\n# Load LLM inference endpoints from an env variable or a file\\\\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\\\\n# and OAI_CONFIG_LIST_sample\\\\nconfig_list = config_list_from_json(env_or_file=\\\\\"OAI_CONFIG_LIST\\\\\")\\\\n# You can also set config_list directly as a list, for example, config_list = [{\\'model\\': \\'gpt-4\\', \\'api_key\\': \\'<your OpenAI API key here>\\'},]\\\\nassistant = AssistantAgent(\\\\\"assistant\\\\\", llm_config={\\\\\"config_list\\\\\": config_list})\\\\nuser_proxy = UserProxyAgent(\\\\\"user_proxy\\\\\", code_execution_config={\\\\\"work_dir\\\\\": \\\\\"coding\\\\\", \\\\\"use_docker\\\\\": False}) # IMPORTANT: set to True to run code in docker, recommended\\\\nuser_proxy.initiate_chat(assistant, message=\\\\\"Plot a chart of NVDA and TESLA stock price change YTD.\\\\\")\\\\n# This initiates an automated chat between the two agents to solve the task\\\\n```\\\\n\\\\nThis example can be run with\\\\n\\\\n```python\\\\npython test/twoagent.py\\\\n```\\\\n\\\\nAfter the repo is cloned.\\\\nThe figure below shows an example conversation flow with AutoGen.\\\\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\\\\n\\\\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\\\\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Enhanced LLM Inferences\\\\n\\\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\\\\n\\\\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\\\\n\\\\n```python\\\\n# perform tuning for openai<1\\\\nconfig, analysis = autogen.Completion.tune(\\\\n    data=tune_data,\\\\n    metric=\\\\\"success\\\\\",\\\\n    mode=\\\\\"max\\\\\",\\\\n    eval_func=eval_func,\\\\n    inference_budget=0.05,\\\\n    optimization_budget=3,\\\\n    num_samples=-1,\\\\n)\\\\n# perform inference for a test instance\\\\nresponse = autogen.Completion.create(context=test_instance, **config)\\\\n```\\\\n\\\\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Documentation\\\\n\\\\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\\\\n\\\\nIn addition, you can find:\\\\n\\\\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\\\\n\\\\n- [Discord](https://aka.ms/autogen-dc)\\\\n\\\\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\\\\n\\\\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Related Papers\\\\n\\\\n[AutoGen](https://arxiv.org/abs/2308.08155)\\\\n\\\\n```\\\\n@inproceedings{wu2023autogen,\\\\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\\\\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\\\\n      year={2023},\\\\n      eprint={2308.08155},\\\\n      archivePrefix={arXiv},\\\\n      primaryClass={cs.AI}\\\\n}\\\\n```\\\\n\\\\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\\\\n\\\\n```\\\\n@inproceedings{wang2023EcoOptiGen,\\\\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\\\\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\\\\n    year={2023},\\\\n    booktitle={AutoML\\'23},\\\\n}\\\\n```\\\\n\\\\n[MathChat](https://arxiv.org/abs/2306.01337)\\\\n\\\\n\", \"role\": \"user\"}], \"model\": \"gpt4-0125preview-128k\", \"temperature\": 0}', 1, 1713883494.872468, None, 1713883494.872468, 0, None, 0, 4, None, b'\\x80\\x05\\x95\\xd9\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x8c!openai.types.chat.chat_completion\\x94\\x8c\\x0eChatCompletion\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08__dict__\\x94}\\x94(\\x8c\\x02id\\x94\\x8c&chatcmpl-9HBg09BM2oOi9vIafmSp6usctCuqe\\x94\\x8c\\x07choices\\x94]\\x94h\\x00\\x8c\\x06Choice\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\rfinish_reason\\x94\\x8c\\x04stop\\x94\\x8c\\x05index\\x94K\\x00\\x8c\\x08logprobs\\x94N\\x8c\\x07message\\x94\\x8c)openai.types.chat.chat_completion_message\\x94\\x8c\\x15ChatCompletionMessage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x07content\\x94\\x8c\\xcbAutoGen is a framework for developing LLM applications using multiple agents that converse with each other to solve tasks, enabling advanced, efficient, and customizable coding and development processes.\\x94\\x8c\\x04role\\x94\\x8c\\tassistant\\x94\\x8c\\rfunction_call\\x94N\\x8c\\ntool_calls\\x94Nu\\x8c\\x12__pydantic_extra__\\x94}\\x94\\x8c\\x17__pydantic_fields_set__\\x94\\x8f\\x94(h\\x1dh\\x1b\\x90\\x8c\\x14__pydantic_private__\\x94Nubuh!}\\x94\\x8c\\x16content_filter_results\\x94}\\x94(\\x8c\\x04hate\\x94}\\x94(\\x8c\\x08filtered\\x94\\x89\\x8c\\x08severity\\x94\\x8c\\x04safe\\x94u\\x8c\\tself_harm\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x06sexual\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x08violence\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uush#\\x8f\\x94(h\\x13h\\x14h\\x10h\\x12\\x90h%Nuba\\x8c\\x07created\\x94J`\\xc9\\'f\\x8c\\x05model\\x94\\x8c\\x05gpt-4\\x94\\x8c\\x06object\\x94\\x8c\\x0fchat.completion\\x94\\x8c\\x12system_fingerprint\\x94\\x8c\\rfp_1402c60a5a\\x94\\x8c\\x05usage\\x94\\x8c\\x1dopenai.types.completion_usage\\x94\\x8c\\x0fCompletionUsage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x11completion_tokens\\x94K\"\\x8c\\rprompt_tokens\\x94M)\\x19\\x8c\\x0ctotal_tokens\\x94MK\\x19uh!}\\x94h#\\x8f\\x94(hFhHhG\\x90h%Nubuh!}\\x94(\\x8c\\x15prompt_filter_results\\x94]\\x94}\\x94(\\x8c\\x0cprompt_index\\x94K\\x00h\\'}\\x94(h)}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh.}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh1}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh4}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uuua\\x8c\\x04cost\\x94G?\\xc8\\xfe\\x9b{\\xf1\\xe8\\xe5uh#\\x8f\\x94(h;h=h\\x07h\\th?h9h8\\x90h%Nub.')\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++ ROW 2 ++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "(2, '{\"messages\": [{\"content\": \"You are a helpful assistant.\", \"role\": \"system\"}, {\"content\": \"What is Autogen?\", \"role\": \"user\"}], \"model\": \"gpt4-0125preview-128k\", \"temperature\": 0}', 1, 1713883550.986583, None, 1713883550.986583, 0, None, 0, 4, None, b'\\x80\\x05\\x95\\xec\\n\\x00\\x00\\x00\\x00\\x00\\x00\\x8c!openai.types.chat.chat_completion\\x94\\x8c\\x0eChatCompletion\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08__dict__\\x94}\\x94(\\x8c\\x02id\\x94\\x8c&chatcmpl-9HBgaM63a0BZkPC53DAQHHd8SoxQt\\x94\\x8c\\x07choices\\x94]\\x94h\\x00\\x8c\\x06Choice\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\rfinish_reason\\x94\\x8c\\x04stop\\x94\\x8c\\x05index\\x94K\\x00\\x8c\\x08logprobs\\x94N\\x8c\\x07message\\x94\\x8c)openai.types.chat.chat_completion_message\\x94\\x8c\\x15ChatCompletionMessage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x07content\\x94X\\xdb\\x06\\x00\\x00\"Autogen\" can refer to different concepts depending on the context in which it\\'s used. Here are a few possibilities:\\n\\n1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or scripts that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, or even documentation from a set of specifications or templates. This automation can significantly speed up development processes and ensure consistency across large projects.\\n\\n2. **Medical or Biological Context**: The term could also be used in a medical or biological context, referring to autogenous or autogenic training or therapy. This is a technique that teaches your body to respond to your verbal commands, helping to control bodily functions that are normally considered automatic, such as heart rate or blood pressure. It\\'s a form of self-hypnosis or relaxation technique.\\n\\n3. **Specific Software or Libraries**: There are specific software packages or libraries named \"Autogen\" designed for various purposes, such as generating code. An example is the GNU AutoGen, a tool designed to simplify the creation and maintenance of programs that contain large amounts of repetitive text. It is especially useful for creating the code and documentation that can be automatically generated from a set of templates.\\n\\n4. **Other Industries**: In other contexts, \"Autogen\" might refer to companies, products, or technologies that emphasize \"auto-generation\" or automation in their processes or functionalities.\\n\\nWithout more specific context, it\\'s challenging to provide a precise definition. If you have a particular area or usage in mind, please provide more details for a more targeted explanation.\\x94\\x8c\\x04role\\x94\\x8c\\tassistant\\x94\\x8c\\rfunction_call\\x94N\\x8c\\ntool_calls\\x94Nu\\x8c\\x12__pydantic_extra__\\x94}\\x94\\x8c\\x17__pydantic_fields_set__\\x94\\x8f\\x94(h\\x1dh\\x1b\\x90\\x8c\\x14__pydantic_private__\\x94Nubuh!}\\x94\\x8c\\x16content_filter_results\\x94}\\x94(\\x8c\\x04hate\\x94}\\x94(\\x8c\\x08filtered\\x94\\x89\\x8c\\x08severity\\x94\\x8c\\x04safe\\x94u\\x8c\\tself_harm\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x06sexual\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x08violence\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uush#\\x8f\\x94(h\\x13h\\x14h\\x10h\\x12\\x90h%Nuba\\x8c\\x07created\\x94J\\x84\\xc9\\'f\\x8c\\x05model\\x94\\x8c\\x05gpt-4\\x94\\x8c\\x06object\\x94\\x8c\\x0fchat.completion\\x94\\x8c\\x12system_fingerprint\\x94\\x8c\\rfp_1402c60a5a\\x94\\x8c\\x05usage\\x94\\x8c\\x1dopenai.types.completion_usage\\x94\\x8c\\x0fCompletionUsage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x11completion_tokens\\x94MD\\x01\\x8c\\rprompt_tokens\\x94K\\x16\\x8c\\x0ctotal_tokens\\x94MZ\\x01uh!}\\x94h#\\x8f\\x94(hFhHhG\\x90h%Nubuh!}\\x94(\\x8c\\x15prompt_filter_results\\x94]\\x94}\\x94(\\x8c\\x0cprompt_index\\x94K\\x00h\\'}\\x94(h)}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh.}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh1}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh4}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uuua\\x8c\\x04cost\\x94G?\\x94\\x95\\x18*\\x990\\xbduh#\\x8f\\x94(h;h=h\\x07h\\th?h9h8\\x90h%Nub.')\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++ ROW 3 ++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "(3, '{\"messages\": [{\"content\": \"\", \"role\": \"system\"}, {\"content\": \"What is Autogen?\", \"role\": \"assistant\"}, {\"content\": \"\\\\\"Autogen\\\\\" can refer to different concepts depending on the context in which it\\'s used. Here are a few possibilities:\\\\n\\\\n1. **Software Development**: In the context of programming and software development, \\\\\"Autogen\\\\\" often refers to tools or scripts that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, or even documentation from a set of specifications or templates. This automation can significantly speed up development processes and ensure consistency across large projects.\\\\n\\\\n2. **Medical or Biological Context**: The term could also be used in a medical or biological context, referring to autogenous or autogenic training or therapy. This is a technique that teaches your body to respond to your verbal commands, helping to control bodily functions that are normally considered automatic, such as heart rate or blood pressure. It\\'s a form of self-hypnosis or relaxation technique.\\\\n\\\\n3. **Specific Software or Libraries**: There are specific software packages or libraries named \\\\\"Autogen\\\\\" designed for various purposes, such as generating code. An example is the GNU AutoGen, a tool designed to simplify the creation and maintenance of programs that contain large amounts of repetitive text. It is especially useful for creating the code and documentation that can be automatically generated from a set of templates.\\\\n\\\\n4. **Other Industries**: In other contexts, \\\\\"Autogen\\\\\" might refer to companies, products, or technologies that emphasize \\\\\"auto-generation\\\\\" or automation in their processes or functionalities.\\\\n\\\\nWithout more specific context, it\\'s challenging to provide a precise definition. If you have a particular area or usage in mind, please provide more details for a more targeted explanation.\", \"role\": \"user\"}], \"model\": \"gpt4-0125preview-128k\", \"temperature\": 0}', 1, 1713883552.423823, None, 1713883552.423823, 0, None, 0, 4, None, b\"\\x80\\x05\\x95\\x1e\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x8c!openai.types.chat.chat_completion\\x94\\x8c\\x0eChatCompletion\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08__dict__\\x94}\\x94(\\x8c\\x02id\\x94\\x8c&chatcmpl-9HBgymcWLpcIVE9qi4VKyNyE6cp0e\\x94\\x8c\\x07choices\\x94]\\x94h\\x00\\x8c\\x06Choice\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\rfinish_reason\\x94\\x8c\\x04stop\\x94\\x8c\\x05index\\x94K\\x00\\x8c\\x08logprobs\\x94N\\x8c\\x07message\\x94\\x8c)openai.types.chat.chat_completion_message\\x94\\x8c\\x15ChatCompletionMessage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x07content\\x94\\x8c\\x10What is Autogen?\\x94\\x8c\\x04role\\x94\\x8c\\tassistant\\x94\\x8c\\rfunction_call\\x94N\\x8c\\ntool_calls\\x94Nu\\x8c\\x12__pydantic_extra__\\x94}\\x94\\x8c\\x17__pydantic_fields_set__\\x94\\x8f\\x94(h\\x1dh\\x1b\\x90\\x8c\\x14__pydantic_private__\\x94Nubuh!}\\x94\\x8c\\x16content_filter_results\\x94}\\x94(\\x8c\\x04hate\\x94}\\x94(\\x8c\\x08filtered\\x94\\x89\\x8c\\x08severity\\x94\\x8c\\x04safe\\x94u\\x8c\\tself_harm\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x06sexual\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x08violence\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uush#\\x8f\\x94(h\\x13h\\x14h\\x10h\\x12\\x90h%Nuba\\x8c\\x07created\\x94J\\x9c\\xc9'f\\x8c\\x05model\\x94\\x8c\\x05gpt-4\\x94\\x8c\\x06object\\x94\\x8c\\x0fchat.completion\\x94\\x8c\\x12system_fingerprint\\x94\\x8c\\rfp_1402c60a5a\\x94\\x8c\\x05usage\\x94\\x8c\\x1dopenai.types.completion_usage\\x94\\x8c\\x0fCompletionUsage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x11completion_tokens\\x94K\\x05\\x8c\\rprompt_tokens\\x94MX\\x01\\x8c\\x0ctotal_tokens\\x94M]\\x01uh!}\\x94h#\\x8f\\x94(hFhHhG\\x90h%Nubuh!}\\x94(\\x8c\\x15prompt_filter_results\\x94]\\x94}\\x94(\\x8c\\x0cprompt_index\\x94K\\x00h'}\\x94(h)}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh.}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh1}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh4}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uuua\\x8c\\x04cost\\x94G?\\x85\\xbf\\xf0Ew\\xd9Vuh#\\x8f\\x94(h;h=h\\x07h\\th?h9h8\\x90h%Nub.\")\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++ ROW 4 ++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "(4, '{\"messages\": [{\"content\": \"You are a helpful assistant.\", \"role\": \"system\"}, {\"content\": \"You\\'re a retrieve augmented chatbot. You answer user\\'s questions based on your own knowledge and the\\\\ncontext provided by the user.\\\\nIf you can\\'t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\\\nYou must give as short an answer as possible.\\\\n\\\\nUser\\'s question is: what are the enhanced LLM inference features?\\\\n\\\\nContext is: \\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\\\\n### Option 1. Install and Run AutoGen in Docker\\\\n\\\\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\\\\n\\\\n### Option 2. Install AutoGen Locally\\\\n\\\\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\\\\n\\\\n```bash\\\\npip install pyautogen\\\\n```\\\\n\\\\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\\\\n\\\\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\\\\n```bash\\\\npip install \\\\\"pyautogen[blendsearch]\\\\\"\\\\n``` -->\\\\n\\\\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\\\\n\\\\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\\\\n\\\\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\\\\n\\\\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Multi-Agent Conversation Framework\\\\n\\\\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\\\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\\\\n\\\\nFeatures of this use case include:\\\\n\\\\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\\\\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\\\\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\\\\n\\\\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\\\n\\\\n```python\\\\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\\\\n# Load LLM inference endpoints from an env variable or a file\\\\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\\\\n# and OAI_CONFIG_LIST_sample\\\\nconfig_list = config_list_from_json(env_or_file=\\\\\"OAI_CONFIG_LIST\\\\\")\\\\n# You can also set config_list directly as a list, for example, config_list = [{\\'model\\': \\'gpt-4\\', \\'api_key\\': \\'<your OpenAI API key here>\\'},]\\\\nassistant = AssistantAgent(\\\\\"assistant\\\\\", llm_config={\\\\\"config_list\\\\\": config_list})\\\\nuser_proxy = UserProxyAgent(\\\\\"user_proxy\\\\\", code_execution_config={\\\\\"work_dir\\\\\": \\\\\"coding\\\\\", \\\\\"use_docker\\\\\": False}) # IMPORTANT: set to True to run code in docker, recommended\\\\nuser_proxy.initiate_chat(assistant, message=\\\\\"Plot a chart of NVDA and TESLA stock price change YTD.\\\\\")\\\\n# This initiates an automated chat between the two agents to solve the task\\\\n```\\\\n\\\\nThis example can be run with\\\\n\\\\n```python\\\\npython test/twoagent.py\\\\n```\\\\n\\\\nAfter the repo is cloned.\\\\nThe figure below shows an example conversation flow with AutoGen.\\\\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\\\\n\\\\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\\\\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Enhanced LLM Inferences\\\\n\\\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\\\\n\\\\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\\\\n\\\\n```python\\\\n# perform tuning for openai<1\\\\nconfig, analysis = autogen.Completion.tune(\\\\n    data=tune_data,\\\\n    metric=\\\\\"success\\\\\",\\\\n    mode=\\\\\"max\\\\\",\\\\n    eval_func=eval_func,\\\\n    inference_budget=0.05,\\\\n    optimization_budget=3,\\\\n    num_samples=-1,\\\\n)\\\\n# perform inference for a test instance\\\\nresponse = autogen.Completion.create(context=test_instance, **config)\\\\n```\\\\n\\\\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Documentation\\\\n\\\\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\\\\n\\\\nIn addition, you can find:\\\\n\\\\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\\\\n\\\\n- [Discord](https://aka.ms/autogen-dc)\\\\n\\\\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\\\\n\\\\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Related Papers\\\\n\\\\n[AutoGen](https://arxiv.org/abs/2308.08155)\\\\n\\\\n```\\\\n@inproceedings{wu2023autogen,\\\\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\\\\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\\\\n      year={2023},\\\\n      eprint={2308.08155},\\\\n      archivePrefix={arXiv},\\\\n      primaryClass={cs.AI}\\\\n}\\\\n```\\\\n\\\\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\\\\n\\\\n```\\\\n@inproceedings{wang2023EcoOptiGen,\\\\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\\\\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\\\\n    year={2023},\\\\n    booktitle={AutoML\\'23},\\\\n}\\\\n```\\\\n\\\\n[MathChat](https://arxiv.org/abs/2306.01337)\\\\n# Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\\\\n\\\\nHarnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\\\\n================================================================================================\\\\n\\\\n[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\\\\n\\\\n\\\\u00b7[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\\\\n\\\\n3 min read\\\\u00b7Dec 19, 2023--\\\\n\\\\nListen\\\\n\\\\nShare\\\\n\\\\n![]()Introduction\\\\n============\\\\n\\\\nIn the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI\\\\u2019s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\\\\n\\\\nStep 1: Setting Up Your Environment\\\\n===================================\\\\n\\\\nBefore diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\\\\n\\\\n```\\\\n!pip install pyautogen==0.2.0b5\\\\n```\\\\nStep 2: Importing the Required Modules\\\\n======================================\\\\n\\\\nOnce the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\\\\n\\\\n```\\\\nfrom autogen import config_list_from_json  \\\\nfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \\\\nfrom autogen import UserProxyAgent  \\\\n  \\\\n# config_list = config_list_from_json(\\\\\"OAI_CONFIG_LIST\\\\\")  \\\\n  \\\\nconfig_list = [  \\\\n    {  \\\\n        \\'model\\': \\'gpt-4\\',  \\\\n        \\'api_key\\': \\'<put your openai key here>\\',  \\\\n    },  \\\\n]\\\\n```\\\\nStep 3: Creating the GPTAssistantAgent\\\\n======================================\\\\n\\\\nThe next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\\\\n\\\\n```\\\\ngpt_assistant = GPTAssistantAgent(  \\\\n    name=\\\\\"assistant\\\\\",  \\\\n    llm_config={  \\\\n        \\\\\"config_list\\\\\": config_list,  \\\\n        \\\\\"assistant_id\\\\\": None  \\\\n    })\\\\n```\\\\nStep 4: Setting Up the UserProxyAgent\\\\n=====================================\\\\n\\\\nThe `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\\\\n\\\\n```\\\\nuser_proxy = UserProxyAgent(  \\\\n    name=\\\\\"user_proxy\\\\\",  \\\\n    code_execution_config={  \\\\n        \\\\\"work_dir\\\\\": \\\\\"coding\\\\\"  \\\\n    },  \\\\n    human_input_mode=\\\\\"NEVER\\\\\")\\\\n```\\\\nStep 5: Initiating the Task\\\\n===========================\\\\n\\\\nWith both agents configured, you can now initiate a task. For example, to print \\\\u201cHello World\\\\u201d, you would do the following:\\\\n\\\\n```\\\\nuser_proxy.initiate_chat(gpt_assistant, message=\\\\\"Print hello world\\\\\")\\\\n```\\\\nStep 6: Enabling Advanced Features\\\\n==================================\\\\n\\\\nFor more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\\\\n\\\\n```\\\\ngpt_assistant = GPTAssistantAgent(  \\\\n    name=\\\\\"assistant\\\\\",  \\\\n    llm_config={  \\\\n        \\\\\"config_list\\\\\": config_list,  \\\\n        \\\\\"assistant_id\\\\\": None,  \\\\n        \\\\\"tools\\\\\": [{\\\\\"type\\\\\": \\\\\"code_interpreter\\\\\"}],  \\\\n    })\\\\n```\\\\nthen test it with this:\\\\n```\\\\nuser_proxy.initiate_chat(gpt_assistant, message=\\\\\"Get the price of gold and print a visual of the last 4 days closing price\\\\\")\\\\n```\\\\n![]()Conclusion\\\\n==========\\\\n\\\\nThe integration of AutoGen and OpenAI\\\\u2019s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\\\\n\\\\nFuture Perspectives\\\\n===================\\\\n\\\\nWhile there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\\\\n\\\\n[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\\\\n\\\\n--\\\\n\\\\n[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\\\\n--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.5K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\\\\n\\\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\\\\n<a name=\\\\\"readme-top\\\\\"></a>\\\\n\\\\n[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\\\\n[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\\\\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\\\\n[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\\\\n[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\\\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\\\\n\\\\n\\\\n# AutoGen\\\\n[\\\\ud83d\\\\udcda Cite paper](#related-papers).\\\\n<!-- <p align=\\\\\"center\\\\\">\\\\n    <img src=\\\\\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\\\\\"  width=200>\\\\n    <br>\\\\n</p> -->\\\\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What\\'s next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital\\'s AI Ascent (Mar 26).\\\\n\\\\n:fire: Mar 3, 2024: What\\'s new in AutoGen? \\\\ud83d\\\\udcf0[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); \\\\ud83d\\\\udcfa[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\\\\n\\\\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\\\\n\\\\n:tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD).\\\\n\\\\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\\\\n\\\\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\\\\n\\\\n<!-- :fire: Nov 11: OpenAI\\'s Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\\\\n\\\\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff.\\\\n\\\\n:tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U).\\\\n\\\\n:tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023.\\\\n\\\\n:tada: Oct 03, 2023: AutoGen spins off from FLAML on GitHub and has a major paper update (first version on Aug 16).\\\\n\\\\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\\\\n\\\\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\\\\n\\\\n<!--\\\\n:fire: FLAML is highlighted in OpenAI\\'s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\\\\n\\\\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\\\\n\\\\n:fire: FLAML supports Code-First AutoML & Tuning \\\\u2013 Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## What is AutoGen\\\\n\\\\nAutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\\\n\\\\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\\\\n\\\\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\\\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\\\n  the number of agents, and agent conversation topology.\\\\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\\\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\\\n\\\\nAutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Roadmaps\\\\n\\\\nTo see what we are working on and what we plan to work on, please check our\\\\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Quickstart\\\\nThe easiest way to start playing is\\\\n1. Click below to use the GitHub Codespace\\\\n\\\\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\\\\n\\\\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\\\\n 3. Start playing with the notebooks!\\\\n\\\\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\\\\n\\\\n\", \"role\": \"user\"}], \"model\": \"gpt4-0125preview-128k\", \"temperature\": 0}', 1, 1713883707.387234, None, 1713883707.387234, 0, None, 0, 4, None, b\"\\x80\\x05\\x95J\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x8c!openai.types.chat.chat_completion\\x94\\x8c\\x0eChatCompletion\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08__dict__\\x94}\\x94(\\x8c\\x02id\\x94\\x8c&chatcmpl-9HBjSWCIMyLjSAWKYeOupQYjNtfRl\\x94\\x8c\\x07choices\\x94]\\x94h\\x00\\x8c\\x06Choice\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\rfinish_reason\\x94\\x8c\\x04stop\\x94\\x8c\\x05index\\x94K\\x00\\x8c\\x08logprobs\\x94N\\x8c\\x07message\\x94\\x8c)openai.types.chat.chat_completion_message\\x94\\x8c\\x15ChatCompletionMessage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x07content\\x94\\x8c<Caching, error handling, multi-config inference, templating.\\x94\\x8c\\x04role\\x94\\x8c\\tassistant\\x94\\x8c\\rfunction_call\\x94N\\x8c\\ntool_calls\\x94Nu\\x8c\\x12__pydantic_extra__\\x94}\\x94\\x8c\\x17__pydantic_fields_set__\\x94\\x8f\\x94(h\\x1dh\\x1b\\x90\\x8c\\x14__pydantic_private__\\x94Nubuh!}\\x94\\x8c\\x16content_filter_results\\x94}\\x94(\\x8c\\x04hate\\x94}\\x94(\\x8c\\x08filtered\\x94\\x89\\x8c\\x08severity\\x94\\x8c\\x04safe\\x94u\\x8c\\tself_harm\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x06sexual\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x08violence\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uush#\\x8f\\x94(h\\x13h\\x14h\\x10h\\x12\\x90h%Nuba\\x8c\\x07created\\x94J6\\xca'f\\x8c\\x05model\\x94\\x8c\\x05gpt-4\\x94\\x8c\\x06object\\x94\\x8c\\x0fchat.completion\\x94\\x8c\\x12system_fingerprint\\x94\\x8c\\rfp_1402c60a5a\\x94\\x8c\\x05usage\\x94\\x8c\\x1dopenai.types.completion_usage\\x94\\x8c\\x0fCompletionUsage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x11completion_tokens\\x94K\\r\\x8c\\rprompt_tokens\\x94M.\\x19\\x8c\\x0ctotal_tokens\\x94M;\\x19uh!}\\x94h#\\x8f\\x94(hFhHhG\\x90h%Nubuh!}\\x94(\\x8c\\x15prompt_filter_results\\x94]\\x94}\\x94(\\x8c\\x0cprompt_index\\x94K\\x00h'}\\x94(h)}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh.}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh1}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh4}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uuua\\x8c\\x04cost\\x94G?\\xc8\\xda<!\\x18~|uh#\\x8f\\x94(h;h=h\\x07h\\th?h9h8\\x90h%Nub.\")\n",
      "\n",
      "\n",
      "\n",
      "++++++++++++++++ ROW 5 ++++++++++++++++++\n",
      "\n",
      "\n",
      "\n",
      "(5, '{\"messages\": [{\"content\": \"You are a helpful assistant.\", \"role\": \"system\"}, {\"content\": \"You\\'re a retrieve augmented chatbot. You answer user\\'s questions based on your own knowledge and the\\\\ncontext provided by the user.\\\\nIf you can\\'t answer the question with or without the current context, you should reply exactly `UPDATE CONTEXT`.\\\\nYou must give as short an answer as possible.\\\\n\\\\nUser\\'s question is: What Python versions are supported by Autogen?\\\\n\\\\nContext is: <a name=\\\\\"readme-top\\\\\"></a>\\\\n\\\\n[![PyPI version](https://badge.fury.io/py/pyautogen.svg)](https://badge.fury.io/py/pyautogen)\\\\n[![Build](https://github.com/microsoft/autogen/actions/workflows/python-package.yml/badge.svg)](https://github.com/microsoft/autogen/actions/workflows/python-package.yml)\\\\n![Python Version](https://img.shields.io/badge/3.8%20%7C%203.9%20%7C%203.10%20%7C%203.11%20%7C%203.12-blue)\\\\n[![Downloads](https://static.pepy.tech/badge/pyautogen/week)](https://pepy.tech/project/pyautogen)\\\\n[![Discord](https://img.shields.io/discord/1153072414184452236?logo=discord&style=flat)](https://aka.ms/autogen-dc)\\\\n[![Twitter](https://img.shields.io/twitter/url/https/twitter.com/cloudposse.svg?style=social&label=Follow%20%40pyautogen)](https://twitter.com/pyautogen)\\\\n\\\\n\\\\n# AutoGen\\\\n[\\\\ud83d\\\\udcda Cite paper](#related-papers).\\\\n<!-- <p align=\\\\\"center\\\\\">\\\\n    <img src=\\\\\"https://github.com/microsoft/autogen/blob/main/website/static/img/flaml.svg\\\\\"  width=200>\\\\n    <br>\\\\n</p> -->\\\\n:fire: Apr 17, 2024: Andrew Ng cited AutoGen in [The Batch newsletter](https://www.deeplearning.ai/the-batch/issue-245/) and [What\\'s next for AI agentic workflows](https://youtu.be/sal78ACtGTc?si=JduUzN_1kDnMq0vF) at Sequoia Capital\\'s AI Ascent (Mar 26).\\\\n\\\\n:fire: Mar 3, 2024: What\\'s new in AutoGen? \\\\ud83d\\\\udcf0[Blog](https://microsoft.github.io/autogen/blog/2024/03/03/AutoGen-Update); \\\\ud83d\\\\udcfa[Youtube](https://www.youtube.com/watch?v=j_mtwQiaLGU).\\\\n\\\\n:fire: Mar 1, 2024: the first AutoGen multi-agent experiment on the challenging [GAIA](https://huggingface.co/spaces/gaia-benchmark/leaderboard) benchmark achieved the No. 1 accuracy in all the three levels.\\\\n\\\\n:tada: Jan 30, 2024: AutoGen is highlighted by Peter Lee in Microsoft Research Forum [Keynote](https://t.co/nUBSjPDjqD).\\\\n\\\\n:tada: Dec 31, 2023: [AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework](https://arxiv.org/abs/2308.08155) is selected by [TheSequence: My Five Favorite AI Papers of 2023](https://thesequence.substack.com/p/my-five-favorite-ai-papers-of-2023).\\\\n\\\\n<!-- :fire: Nov 24: pyautogen [v0.2](https://github.com/microsoft/autogen/releases/tag/v0.2.0) is released with many updates and new features compared to v0.1.1. It switches to using openai-python v1. Please read the [migration guide](https://microsoft.github.io/autogen/docs/Installation#python). -->\\\\n\\\\n<!-- :fire: Nov 11: OpenAI\\'s Assistants are available in AutoGen and interoperatable with other AutoGen agents! Checkout our [blogpost](https://microsoft.github.io/autogen/blog/2023/11/13/OAI-assistants) for details and examples. -->\\\\n\\\\n:tada: Nov 8, 2023: AutoGen is selected into [Open100: Top 100 Open Source achievements](https://www.benchcouncil.org/evaluation/opencs/annual.html) 35 days after spinoff.\\\\n\\\\n:tada: Nov 6, 2023: AutoGen is mentioned by Satya Nadella in a [fireside chat](https://youtu.be/0pLBvgYtv6U).\\\\n\\\\n:tada: Nov 1, 2023: AutoGen is the top trending repo on GitHub in October 2023.\\\\n\\\\n:tada: Oct 03, 2023: AutoGen spins off from FLAML on GitHub and has a major paper update (first version on Aug 16).\\\\n\\\\n<!-- :tada: Aug 16: Paper about AutoGen on [arxiv](https://arxiv.org/abs/2308.08155). -->\\\\n\\\\n:tada: Mar 29, 2023: AutoGen is first created in [FLAML](https://github.com/microsoft/FLAML).\\\\n\\\\n<!--\\\\n:fire: FLAML is highlighted in OpenAI\\'s [cookbook](https://github.com/openai/openai-cookbook#related-resources-from-around-the-web).\\\\n\\\\n:fire: [autogen](https://microsoft.github.io/autogen/) is released with support for ChatGPT and GPT-4, based on [Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference](https://arxiv.org/abs/2303.04673).\\\\n\\\\n:fire: FLAML supports Code-First AutoML & Tuning \\\\u2013 Private Preview in [Microsoft Fabric Data Science](https://learn.microsoft.com/en-us/fabric/data-science/). -->\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## What is AutoGen\\\\n\\\\nAutoGen is a framework that enables the development of LLM applications using multiple agents that can converse with each other to solve tasks. AutoGen agents are customizable, conversable, and seamlessly allow human participation. They can operate in various modes that employ combinations of LLMs, human inputs, and tools.\\\\n\\\\n![AutoGen Overview](https://github.com/microsoft/autogen/blob/main/website/static/img/autogen_agentchat.png)\\\\n\\\\n- AutoGen enables building next-gen LLM applications based on [multi-agent conversations](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) with minimal effort. It simplifies the orchestration, automation, and optimization of a complex LLM workflow. It maximizes the performance of LLM models and overcomes their weaknesses.\\\\n- It supports [diverse conversation patterns](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#supporting-diverse-conversation-patterns) for complex workflows. With customizable and conversable agents, developers can use AutoGen to build a wide range of conversation patterns concerning conversation autonomy,\\\\n  the number of agents, and agent conversation topology.\\\\n- It provides a collection of working systems with different complexities. These systems span a [wide range of applications](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat#diverse-applications-implemented-with-autogen) from various domains and complexities. This demonstrates how AutoGen can easily support diverse conversation patterns.\\\\n- AutoGen provides [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification). It offers utilities like API unification and caching, and advanced usage patterns, such as error handling, multi-config inference, context programming, etc.\\\\n\\\\nAutoGen is powered by collaborative [research studies](https://microsoft.github.io/autogen/docs/Research) from Microsoft, Penn State University, and the University of Washington.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Roadmaps\\\\n\\\\nTo see what we are working on and what we plan to work on, please check our\\\\n[Roadmap Issues](https://aka.ms/autogen-roadmap).\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Quickstart\\\\nThe easiest way to start playing is\\\\n1. Click below to use the GitHub Codespace\\\\n\\\\n    [![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/microsoft/autogen?quickstart=1)\\\\n\\\\n 2. Copy OAI_CONFIG_LIST_sample to ./notebook folder, name to OAI_CONFIG_LIST, and set the correct configuration.\\\\n 3. Start playing with the notebooks!\\\\n\\\\n*NOTE*: OAI_CONFIG_LIST_sample lists GPT-4 as the default model, as this represents our current recommendation, and is known to work well with AutoGen. If you use a model other than GPT-4, you may need to revise various system prompts (especially if using weaker models like GPT-3.5-turbo). Moreover, if you use models other than those hosted by OpenAI or Azure, you may incur additional risks related to alignment and safety. Proceed with caution if updating this default.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## [Installation](https://microsoft.github.io/autogen/docs/Installation)\\\\n### Option 1. Install and Run AutoGen in Docker\\\\n\\\\nFind detailed instructions for users [here](https://microsoft.github.io/autogen/docs/installation/Docker#step-1-install-docker), and for developers [here](https://microsoft.github.io/autogen/docs/Contribute#docker-for-development).\\\\n\\\\n### Option 2. Install AutoGen Locally\\\\n\\\\nAutoGen requires **Python version >= 3.8, < 3.13**. It can be installed from pip:\\\\n\\\\n```bash\\\\npip install pyautogen\\\\n```\\\\n\\\\nMinimal dependencies are installed without extra options. You can install extra options based on the feature you need.\\\\n\\\\n<!-- For example, use the following to install the dependencies needed by the [`blendsearch`](https://microsoft.github.io/FLAML/docs/Use-Cases/Tune-User-Defined-Function#blendsearch-economical-hyperparameter-optimization-with-blended-search-strategy) option.\\\\n```bash\\\\npip install \\\\\"pyautogen[blendsearch]\\\\\"\\\\n``` -->\\\\n\\\\nFind more options in [Installation](https://microsoft.github.io/autogen/docs/Installation#option-2-install-autogen-locally-using-virtual-environment).\\\\n\\\\n<!-- Each of the [`notebook examples`](https://github.com/microsoft/autogen/tree/main/notebook) may require a specific option to be installed. -->\\\\n\\\\nEven if you are installing and running AutoGen locally outside of docker, the recommendation and default behavior of agents is to perform [code execution](https://microsoft.github.io/autogen/docs/FAQ/#code-execution) in docker. Find more instructions and how to change the default behaviour [here](https://microsoft.github.io/autogen/docs/Installation#code-execution-with-docker-(default)).\\\\n\\\\nFor LLM inference configurations, check the [FAQs](https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints).\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Multi-Agent Conversation Framework\\\\n\\\\nAutogen enables the next-gen LLM applications with a generic [multi-agent conversation](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat) framework. It offers customizable and conversable agents that integrate LLMs, tools, and humans.\\\\nBy automating chat among multiple capable agents, one can easily make them collectively perform tasks autonomously or with human feedback, including tasks that require using tools via code.\\\\n\\\\nFeatures of this use case include:\\\\n\\\\n- **Multi-agent conversations**: AutoGen agents can communicate with each other to solve tasks. This allows for more complex and sophisticated applications than would be possible with a single LLM.\\\\n- **Customization**: AutoGen agents can be customized to meet the specific needs of an application. This includes the ability to choose the LLMs to use, the types of human input to allow, and the tools to employ.\\\\n- **Human participation**: AutoGen seamlessly allows human participation. This means that humans can provide input and feedback to the agents as needed.\\\\n\\\\nFor [example](https://github.com/microsoft/autogen/blob/main/test/twoagent.py),\\\\n\\\\n```python\\\\nfrom autogen import AssistantAgent, UserProxyAgent, config_list_from_json\\\\n# Load LLM inference endpoints from an env variable or a file\\\\n# See https://microsoft.github.io/autogen/docs/FAQ#set-your-api-endpoints\\\\n# and OAI_CONFIG_LIST_sample\\\\nconfig_list = config_list_from_json(env_or_file=\\\\\"OAI_CONFIG_LIST\\\\\")\\\\n# You can also set config_list directly as a list, for example, config_list = [{\\'model\\': \\'gpt-4\\', \\'api_key\\': \\'<your OpenAI API key here>\\'},]\\\\nassistant = AssistantAgent(\\\\\"assistant\\\\\", llm_config={\\\\\"config_list\\\\\": config_list})\\\\nuser_proxy = UserProxyAgent(\\\\\"user_proxy\\\\\", code_execution_config={\\\\\"work_dir\\\\\": \\\\\"coding\\\\\", \\\\\"use_docker\\\\\": False}) # IMPORTANT: set to True to run code in docker, recommended\\\\nuser_proxy.initiate_chat(assistant, message=\\\\\"Plot a chart of NVDA and TESLA stock price change YTD.\\\\\")\\\\n# This initiates an automated chat between the two agents to solve the task\\\\n```\\\\n\\\\nThis example can be run with\\\\n\\\\n```python\\\\npython test/twoagent.py\\\\n```\\\\n\\\\nAfter the repo is cloned.\\\\nThe figure below shows an example conversation flow with AutoGen.\\\\n![Agent Chat Example](https://github.com/microsoft/autogen/blob/main/website/static/img/chat_example.png)\\\\n\\\\nAlternatively, the [sample code](https://github.com/microsoft/autogen/blob/main/samples/simple_chat.py) here allows a user to chat with an AutoGen agent in ChatGPT style.\\\\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#automated-multi-agent-chat) for this feature.\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Enhanced LLM Inferences\\\\n\\\\nAutogen also helps maximize the utility out of the expensive LLMs such as ChatGPT and GPT-4. It offers [enhanced LLM inference](https://microsoft.github.io/autogen/docs/Use-Cases/enhanced_inference#api-unification) with powerful functionalities like caching, error handling, multi-config inference and templating.\\\\n\\\\n<!-- For example, you can optimize generations by LLM with your own tuning data, success metrics, and budgets.\\\\n\\\\n```python\\\\n# perform tuning for openai<1\\\\nconfig, analysis = autogen.Completion.tune(\\\\n    data=tune_data,\\\\n    metric=\\\\\"success\\\\\",\\\\n    mode=\\\\\"max\\\\\",\\\\n    eval_func=eval_func,\\\\n    inference_budget=0.05,\\\\n    optimization_budget=3,\\\\n    num_samples=-1,\\\\n)\\\\n# perform inference for a test instance\\\\nresponse = autogen.Completion.create(context=test_instance, **config)\\\\n```\\\\n\\\\nPlease find more [code examples](https://microsoft.github.io/autogen/docs/Examples#tune-gpt-models) for this feature. -->\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Documentation\\\\n\\\\nYou can find detailed documentation about AutoGen [here](https://microsoft.github.io/autogen/).\\\\n\\\\nIn addition, you can find:\\\\n\\\\n- [Research](https://microsoft.github.io/autogen/docs/Research), [blogposts](https://microsoft.github.io/autogen/blog) around AutoGen, and [Transparency FAQs](https://github.com/microsoft/autogen/blob/main/TRANSPARENCY_FAQS.md)\\\\n\\\\n- [Discord](https://aka.ms/autogen-dc)\\\\n\\\\n- [Contributing guide](https://microsoft.github.io/autogen/docs/Contribute)\\\\n\\\\n- [Roadmap](https://github.com/orgs/microsoft/projects/989/views/3)\\\\n\\\\n<p align=\\\\\"right\\\\\" style=\\\\\"font-size: 14px; color: #555; margin-top: 20px;\\\\\">\\\\n  <a href=\\\\\"#readme-top\\\\\" style=\\\\\"text-decoration: none; color: blue; font-weight: bold;\\\\\">\\\\n    \\\\u2191 Back to Top \\\\u2191\\\\n  </a>\\\\n</p>\\\\n\\\\n## Related Papers\\\\n\\\\n[AutoGen](https://arxiv.org/abs/2308.08155)\\\\n\\\\n```\\\\n@inproceedings{wu2023autogen,\\\\n      title={AutoGen: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework},\\\\n      author={Qingyun Wu and Gagan Bansal and Jieyu Zhang and Yiran Wu and Beibin Li and Erkang Zhu and Li Jiang and Xiaoyun Zhang and Shaokun Zhang and Jiale Liu and Ahmed Hassan Awadallah and Ryen W White and Doug Burger and Chi Wang},\\\\n      year={2023},\\\\n      eprint={2308.08155},\\\\n      archivePrefix={arXiv},\\\\n      primaryClass={cs.AI}\\\\n}\\\\n```\\\\n\\\\n[EcoOptiGen](https://arxiv.org/abs/2303.04673)\\\\n\\\\n```\\\\n@inproceedings{wang2023EcoOptiGen,\\\\n    title={Cost-Effective Hyperparameter Optimization for Large Language Model Generation Inference},\\\\n    author={Chi Wang and Susan Xueqing Liu and Ahmed H. Awadallah},\\\\n    year={2023},\\\\n    booktitle={AutoML\\'23},\\\\n}\\\\n```\\\\n\\\\n[MathChat](https://arxiv.org/abs/2306.01337)\\\\n# Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium\\\\n\\\\nHarnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development | by Dr. Ernesto Lee | Medium[Open in app](https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F571ddb6f814c&%7Efeature=LoOpenInAppButton&%7Echannel=ShowPostUnderUser&source=---two_column_layout_nav----------------------------------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Write](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fmedium.com%2Fnew-story&source=---two_column_layout_nav-----------------------new_post_topnav-----------)[Sign up](https://medium.com/m/signin?operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n[Sign in](https://medium.com/m/signin?operation=login&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&source=post_page---two_column_layout_nav-----------------------global_nav-----------)\\\\n\\\\n![](https://miro.medium.com/v2/resize:fill:64:64/1*dmbNkD5D-u45r44go_cf0g.png)Harnessing the Power of AutoGen and OpenAI GPTs for Advanced Code Interpretation and Development\\\\n================================================================================================\\\\n\\\\n[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:88:88/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Dr. Ernesto Lee](/?source=post_page-----571ddb6f814c--------------------------------)\\\\n\\\\n\\\\u00b7[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------post_header-----------)\\\\n\\\\n3 min read\\\\u00b7Dec 19, 2023--\\\\n\\\\nListen\\\\n\\\\nShare\\\\n\\\\n![]()Introduction\\\\n============\\\\n\\\\nIn the rapidly evolving world of AI and software development, the collaboration between AutoGen and OpenAI\\\\u2019s GPTs is a game-changer. This integration allows for the creation of advanced, multi-agent systems capable of tackling complex coding tasks with unprecedented efficiency and intelligence. In this article, we will delve into how you can utilize AutoGen in conjunction with GPTs for sophisticated code interpretation, debugging, and development.\\\\n\\\\nStep 1: Setting Up Your Environment\\\\n===================================\\\\n\\\\nBefore diving into the practical application, you need to set up your environment. This involves installing the necessary AutoGen package. Execute the following command in your terminal:\\\\n\\\\n```\\\\n!pip install pyautogen==0.2.0b5\\\\n```\\\\nStep 2: Importing the Required Modules\\\\n======================================\\\\n\\\\nOnce the installation is complete, you need to import the necessary modules from AutoGen and configure them. Start by importing `config_list_from_json`, `GPTAssistantAgent`, and `UserProxyAgent`:\\\\n\\\\n```\\\\nfrom autogen import config_list_from_json  \\\\nfrom autogen.agentchat.contrib.gpt_assistant_agent import GPTAssistantAgent  \\\\nfrom autogen import UserProxyAgent  \\\\n  \\\\n# config_list = config_list_from_json(\\\\\"OAI_CONFIG_LIST\\\\\")  \\\\n  \\\\nconfig_list = [  \\\\n    {  \\\\n        \\'model\\': \\'gpt-4\\',  \\\\n        \\'api_key\\': \\'<put your openai key here>\\',  \\\\n    },  \\\\n]\\\\n```\\\\nStep 3: Creating the GPTAssistantAgent\\\\n======================================\\\\n\\\\nThe next step involves defining the OpenAI assistant agent. This agent will act as your primary interface with the GPT models:\\\\n\\\\n```\\\\ngpt_assistant = GPTAssistantAgent(  \\\\n    name=\\\\\"assistant\\\\\",  \\\\n    llm_config={  \\\\n        \\\\\"config_list\\\\\": config_list,  \\\\n        \\\\\"assistant_id\\\\\": None  \\\\n    })\\\\n```\\\\nStep 4: Setting Up the UserProxyAgent\\\\n=====================================\\\\n\\\\nThe `UserProxyAgent` serves as an intermediary for code execution and human input management. Set it up as follows:\\\\n\\\\n```\\\\nuser_proxy = UserProxyAgent(  \\\\n    name=\\\\\"user_proxy\\\\\",  \\\\n    code_execution_config={  \\\\n        \\\\\"work_dir\\\\\": \\\\\"coding\\\\\"  \\\\n    },  \\\\n    human_input_mode=\\\\\"NEVER\\\\\")\\\\n```\\\\nStep 5: Initiating the Task\\\\n===========================\\\\n\\\\nWith both agents configured, you can now initiate a task. For example, to print \\\\u201cHello World\\\\u201d, you would do the following:\\\\n\\\\n```\\\\nuser_proxy.initiate_chat(gpt_assistant, message=\\\\\"Print hello world\\\\\")\\\\n```\\\\nStep 6: Enabling Advanced Features\\\\n==================================\\\\n\\\\nFor more advanced coding tasks, such as code interpretation, you can enhance your GPTAssistantAgent with additional tools:\\\\n\\\\n```\\\\ngpt_assistant = GPTAssistantAgent(  \\\\n    name=\\\\\"assistant\\\\\",  \\\\n    llm_config={  \\\\n        \\\\\"config_list\\\\\": config_list,  \\\\n        \\\\\"assistant_id\\\\\": None,  \\\\n        \\\\\"tools\\\\\": [{\\\\\"type\\\\\": \\\\\"code_interpreter\\\\\"}],  \\\\n    })\\\\n```\\\\nthen test it with this:\\\\n```\\\\nuser_proxy.initiate_chat(gpt_assistant, message=\\\\\"Get the price of gold and print a visual of the last 4 days closing price\\\\\")\\\\n```\\\\n![]()Conclusion\\\\n==========\\\\n\\\\nThe integration of AutoGen and OpenAI\\\\u2019s GPTs marks a significant advancement in AI-driven coding. This combination not only streamlines the coding process but also opens new avenues for tackling complex software development challenges. With these steps, you can start leveraging this powerful duo to enhance your coding projects, be it for debugging, writing new code snippets, or even learning new programming concepts.\\\\n\\\\nFuture Perspectives\\\\n===================\\\\n\\\\nWhile there are limitations, such as the pending integration of group chat managers and the anticipation of multimodal capabilities, the potential for growth and enhancement in this area is immense. The ongoing developments in AutoGen and GPT integration promise a future where coding and software development are more intuitive, efficient, and aligned with the evolving needs of developers.\\\\n\\\\n[Data Science](https://medium.com/tag/data-science?source=post_page-----571ddb6f814c---------------data_science-----------------)[Machine Learning](https://medium.com/tag/machine-learning?source=post_page-----571ddb6f814c---------------machine_learning-----------------)[Artificial Intelligence](https://medium.com/tag/artificial-intelligence?source=post_page-----571ddb6f814c---------------artificial_intelligence-----------------)[Technology](https://medium.com/tag/technology?source=post_page-----571ddb6f814c---------------technology-----------------)[ChatGPT](https://medium.com/tag/chatgpt?source=post_page-----571ddb6f814c---------------chatgpt-----------------)--\\\\n\\\\n--\\\\n\\\\n[![Dr. Ernesto Lee](https://miro.medium.com/v2/resize:fill:144:144/1*nzdYUSs4c2RQs2W0FCHv1g.jpeg)](/?source=post_page-----571ddb6f814c--------------------------------)[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Written by Dr. Ernesto Lee\\\\n--------------------------](/?source=post_page-----571ddb6f814c--------------------------------)[1.5K Followers](/followers?source=post_page-----571ddb6f814c--------------------------------)Miami Dade College Data Analytics Faculty and Chief Innovation Officer [TriveraTech.com](http://TriveraTech.com)\\\\n\\\\n[Follow](https://medium.com/m/signin?actionUrl=https%3A%2F%2Fmedium.com%2F_%2Fsubscribe%2Fuser%2F53ee0651b83e&operation=register&redirect=https%3A%2F%2Fdrlee.io%2Fharnessing-the-power-of-autogen-and-openai-gpts-for-advanced-code-interpretation-and-development-571ddb6f814c&user=Dr.+Ernesto+Lee&userId=53ee0651b83e&source=post_page-53ee0651b83e----571ddb6f814c---------------------follow_profile-----------)[Help](https://help.medium.com/hc/en-us?source=post_page-----571ddb6f814c--------------------------------)[Status](https://medium.statuspage.io/?source=post_page-----571ddb6f814c--------------------------------)[About](https://medium.com/about?autoplay=1&source=post_page-----571ddb6f814c--------------------------------)[Careers](https://medium.com/jobs-at-medium/work-at-medium-959d1a85284e?source=post_page-----571ddb6f814c--------------------------------)[Blog](https://blog.medium.com/?source=post_page-----571ddb6f814c--------------------------------)[Privacy](https://policy.medium.com/medium-privacy-policy-f03bf92035c9?source=post_page-----571ddb6f814c--------------------------------)[Terms](https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----571ddb6f814c--------------------------------)[Text to speech](https://speechify.com/medium?source=post_page-----571ddb6f814c--------------------------------)[Teams](https://medium.com/business?source=post_page-----571ddb6f814c--------------------------------)\\\\n\\\\n\", \"role\": \"user\"}], \"model\": \"gpt4-0125preview-128k\", \"temperature\": 0}', 1, 1713883752.6857903, None, 1713883752.6857903, 0, None, 0, 4, None, b\"\\x80\\x05\\x95,\\x04\\x00\\x00\\x00\\x00\\x00\\x00\\x8c!openai.types.chat.chat_completion\\x94\\x8c\\x0eChatCompletion\\x94\\x93\\x94)\\x81\\x94}\\x94(\\x8c\\x08__dict__\\x94}\\x94(\\x8c\\x02id\\x94\\x8c&chatcmpl-9HBk9DHo1IIPHvXSTE3R3FEBVKFqm\\x94\\x8c\\x07choices\\x94]\\x94h\\x00\\x8c\\x06Choice\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\rfinish_reason\\x94\\x8c\\x04stop\\x94\\x8c\\x05index\\x94K\\x00\\x8c\\x08logprobs\\x94N\\x8c\\x07message\\x94\\x8c)openai.types.chat.chat_completion_message\\x94\\x8c\\x15ChatCompletionMessage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x07content\\x94\\x8c\\x1e3.8 | 3.9 | 3.10 | 3.11 | 3.12\\x94\\x8c\\x04role\\x94\\x8c\\tassistant\\x94\\x8c\\rfunction_call\\x94N\\x8c\\ntool_calls\\x94Nu\\x8c\\x12__pydantic_extra__\\x94}\\x94\\x8c\\x17__pydantic_fields_set__\\x94\\x8f\\x94(h\\x1dh\\x1b\\x90\\x8c\\x14__pydantic_private__\\x94Nubuh!}\\x94\\x8c\\x16content_filter_results\\x94}\\x94(\\x8c\\x04hate\\x94}\\x94(\\x8c\\x08filtered\\x94\\x89\\x8c\\x08severity\\x94\\x8c\\x04safe\\x94u\\x8c\\tself_harm\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x06sexual\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94u\\x8c\\x08violence\\x94}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uush#\\x8f\\x94(h\\x13h\\x14h\\x10h\\x12\\x90h%Nuba\\x8c\\x07created\\x94Ja\\xca'f\\x8c\\x05model\\x94\\x8c\\x05gpt-4\\x94\\x8c\\x06object\\x94\\x8c\\x0fchat.completion\\x94\\x8c\\x12system_fingerprint\\x94\\x8c\\rfp_1402c60a5a\\x94\\x8c\\x05usage\\x94\\x8c\\x1dopenai.types.completion_usage\\x94\\x8c\\x0fCompletionUsage\\x94\\x93\\x94)\\x81\\x94}\\x94(h\\x05}\\x94(\\x8c\\x11completion_tokens\\x94K\\x17\\x8c\\rprompt_tokens\\x94M.\\x19\\x8c\\x0ctotal_tokens\\x94ME\\x19uh!}\\x94h#\\x8f\\x94(hFhHhG\\x90h%Nubuh!}\\x94(\\x8c\\x15prompt_filter_results\\x94]\\x94}\\x94(\\x8c\\x0cprompt_index\\x94K\\x00h'}\\x94(h)}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh.}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh1}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uh4}\\x94(h+\\x89h,\\x8c\\x04safe\\x94uuua\\x8c\\x04cost\\x94G?\\xc8\\xed\\xe5KH\\xd3\\xaeuh#\\x8f\\x94(h;h=h\\x07h\\th?h9h8\\x90h%Nub.\")\n"
     ]
    }
   ],
   "source": [
    "# read the content of the cache\n",
    "\n",
    "import sqlite3\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect(f'./.cache/{cache_seed}/cache.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cur = conn.cursor()\n",
    "\n",
    "# List all tables in the database\n",
    "cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(f\"Tables in this DB: {cur.fetchall()}\\n\")\n",
    "\n",
    "# Execute a query\n",
    "cur.execute(\"SELECT * FROM Cache\")\n",
    "\n",
    "i = 1\n",
    "# Fetch and print the results\n",
    "for row in cur.fetchall():\n",
    "    print(f\"\\n\\n\\n++++++++++++++++ ROW {i} ++++++++++++++++++\\n\\n\\n\")    \n",
    "    print(row)\n",
    "    i=i+1\n",
    "\n",
    "# Close the connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8fb406-3ab9-4c16-8d35-f8d169b81d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name                       = \"user_proxy\",\n",
    "    human_input_mode           = \"NEVER\", # NEVER / ALWAYS / TERMINATE\n",
    "    max_consecutive_auto_reply = 1,\n",
    "\n",
    "    # if the x[\"content\"] ends by \"TERMINATE\", is_termination_msg-->True; otherwise, is_termination_msg--> False\n",
    "    is_termination_msg         = lambda x: x.get(\"content\", \"\").rstrip().endswith(\"TERMINATE\"),\n",
    "    \n",
    "    code_execution_config = {\n",
    "        \"work_dir\": \"coding\",\n",
    "        \n",
    "        # Using docker is safer than running the generated code directly.\n",
    "        # set use_docker=True if docker is available to run the generated code. \n",
    "        \"use_docker\": False\n",
    "    },\n",
    "    \n",
    "    llm_config=llm_config\n",
    ")\n",
    "\n",
    "chat_res = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message = \"What is Autogen?\",\n",
    "    summary_method = \"reflection_with_llm\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8dc4f3-b9c5-4a41-896b-2804ad1e23dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f7e817c-1b35-4ff7-b4e8-55ab655282e9",
   "metadata": {},
   "source": [
    "### assistant (to user_proxy):\n",
    "\n",
    "\"Autogen\" can refer to different concepts depending on the context in which it's used. Here are a few possibilities:\n",
    "\n",
    "1. **Software Development**: In the context of programming and software development, \"Autogen\" often refers to tools or processes that automatically generate code or documentation. These tools can help in creating boilerplate code, APIs, client libraries, or even documentation from a set of definitions or templates. This automation can significantly speed up development and ensure consistency across different parts of a project.\n",
    "\n",
    "2. **Autogenic Training**: In a completely different context, \"Autogen\" could be a shorthand or misspelling for \"autogenic training,\" which is a relaxation technique developed by the German psychiatrist Johannes Heinrich Schultz. Autogenic training involves a series of exercises designed to induce a state of relaxation and reduce stress through self-suggestion and focusing on bodily sensations.\n",
    "\n",
    "3. **Other Contexts**: The term could also be part of a brand name, product, or specific technology in various industries, such as automotive, aerospace, or health and wellness. Without more specific context, it's challenging to pinpoint exactly what \"Autogen\" refers to.\n",
    "\n",
    "If you have a specific context in mind for \"Autogen,\" providing more details could help in giving a more accurate and targeted explanation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "autogen",
   "language": "python",
   "name": "autogen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
